{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaraies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load The dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>169142.0</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>390.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>169347.0</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>169351.0</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>77.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>169966.0</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>245.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>170348.0</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>42.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0          0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1          0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2          1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3          1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4          2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4995  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "4996  169347.0  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
       "4997  169351.0 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346   \n",
       "4998  169966.0 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
       "4999  170348.0  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1    -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2     0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3     0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4     0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4995 -0.882850  0.697211 -2.064945  ...  0.778584 -0.319189  0.639419   \n",
       "4996 -1.413170  0.248525 -1.127396  ...  0.370612  0.028234 -0.145640   \n",
       "4997 -2.234739  1.210158 -0.652250  ...  0.751826  0.834108  0.190944   \n",
       "4998 -2.208002  1.058733 -1.632333  ...  0.583276 -0.269209 -0.456108   \n",
       "4999  0.223050 -0.068384  0.577829  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1    -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2    -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3    -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4     0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "4995 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00      1  \n",
       "4996 -0.081049  0.521875  0.739467  0.389152  0.186637    0.76      1  \n",
       "4997  0.032070 -0.739695  0.471111  0.385107  0.194361   77.89      1  \n",
       "4998 -0.183659 -0.328168  0.606116  0.884876 -0.253700  245.00      1  \n",
       "4999 -0.450261  0.313267 -0.289617  0.002988 -0.015309   42.53      1  \n",
       "\n",
       "[5000 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Datasets\\FinalCreditCard.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is scaled Already, except the Amount and Time colomns are left to scale. so produdre we will follow in Preprocessing phase as follow:\n",
    "* using RobustScaler to scale Time and Amount columns because it is not effected by small number of huge marginal outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the fraud and not fruad data separtely \n",
    "fraud = data[data['Class'] == 1]\n",
    "notfraud = data[data['Class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "#Scale Time and Amount Outliers\n",
    "data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reomeve the coloumns before, becase we already done scaling\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount = data['scaled_amount']\n",
    "scaled_time = data['scaled_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "\n",
    "# Put the new columns in suitable order\n",
    "data.insert(0, 'scaled_amount', scaled_amount)\n",
    "data.insert(1, 'scaled_time', scaled_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.303559</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.210643</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.222793</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856605</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940965</td>\n",
       "      <td>-0.909011</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       2.303559    -0.909889 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.210643    -0.909889  1.191857  0.266151  0.166480  0.448154   \n",
       "2       6.222793    -0.909450 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.856605    -0.909450 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.940965    -0.909011 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the data after is completely scaled \n",
    "fraud = data[data['Class'] == 1]\n",
    "notfraud = data[data['Class'] == 0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase \n",
    "As well as our dataset is clearly unbalanced, we will utilize StratifiedShuffleSplit() function. this function used to split data train/test sets by their indices. StratifiedShuffleSplit() is Cross-Validation object combine between two things, which are: ShuffleSplit and StratifiedKFold, and it produces Stratified Randomize Folds. Those folds are made by maintain the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Frauds 90.16 % of the dataset\n",
      "Frauds 9.84 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# see the percentages of fraud and non fraud  \n",
    "\n",
    "print('Not Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.303559</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.210643</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.222793</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856605</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940965</td>\n",
       "      <td>-0.909011</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>6.416838</td>\n",
       "      <td>73.348480</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391167</td>\n",
       "      <td>1.252967</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.243669</td>\n",
       "      <td>73.438481</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737657</td>\n",
       "      <td>0.226138</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1.076146</td>\n",
       "      <td>73.440237</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266272</td>\n",
       "      <td>0.247968</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>3.935661</td>\n",
       "      <td>73.710240</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035480</td>\n",
       "      <td>0.306271</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.471081</td>\n",
       "      <td>73.877950</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593509</td>\n",
       "      <td>-0.017652</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0          2.303559    -0.909889 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1         -0.210643    -0.909889  1.191857  0.266151  0.166480  0.448154   \n",
       "2          6.222793    -0.909450 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3          1.856605    -0.909450 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4          0.940965    -0.909011 -1.158233  0.877737  1.548718  0.403034   \n",
       "...             ...          ...       ...       ...       ...       ...   \n",
       "4995       6.416838    73.348480 -1.927883  1.125653 -4.518331  1.749293   \n",
       "4996      -0.243669    73.438481  1.378559  1.289381 -5.004247  1.411850   \n",
       "4997       1.076146    73.440237 -0.676143  1.126366 -2.213700  0.468308   \n",
       "4998       3.935661    73.710240 -3.113832  0.585864 -5.399730  1.817092   \n",
       "4999       0.471081    73.877950  1.991976  0.158476 -2.583441  0.408670   \n",
       "\n",
       "            V5        V6        V7        V8  ...       V19       V20  \\\n",
       "0    -0.338321  0.462388  0.239599  0.098698  ...  0.403993  0.251412   \n",
       "1     0.060018 -0.082361 -0.078803  0.085102  ... -0.145783 -0.069083   \n",
       "2    -0.503198  1.800499  0.791461  0.247676  ... -2.261857  0.524980   \n",
       "3    -0.010309  1.247203  0.237609  0.377436  ... -1.232622 -0.208038   \n",
       "4    -0.407193  0.095921  0.592941 -0.270533  ...  0.803487  0.408542   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "4995 -1.566487 -2.010494 -0.882850  0.697211  ...  0.391167  1.252967   \n",
       "4996  0.442581 -1.326536 -1.413170  0.248525  ...  0.737657  0.226138   \n",
       "4997 -1.120541 -0.003346 -2.234739  1.210158  ...  0.266272  0.247968   \n",
       "4998 -0.840618 -2.943548 -2.208002  1.058733  ... -0.035480  0.306271   \n",
       "4999  1.151147 -0.096695  0.223050 -0.068384  ...  0.593509 -0.017652   \n",
       "\n",
       "           V21       V22       V23       V24       V25       V26       V27  \\\n",
       "0    -0.018307  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558   \n",
       "1    -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983   \n",
       "2     0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353   \n",
       "3    -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723   \n",
       "4    -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4995  0.778584 -0.319189  0.639419 -0.294885  0.537503  0.788395  0.292680   \n",
       "4996  0.370612  0.028234 -0.145640 -0.081049  0.521875  0.739467  0.389152   \n",
       "4997  0.751826  0.834108  0.190944  0.032070 -0.739695  0.471111  0.385107   \n",
       "4998  0.583276 -0.269209 -0.456108 -0.183659 -0.328168  0.606116  0.884876   \n",
       "4999 -0.164350 -0.295135 -0.072173 -0.450261  0.313267 -0.289617  0.002988   \n",
       "\n",
       "           V28  \n",
       "0    -0.021053  \n",
       "1     0.014724  \n",
       "2    -0.059752  \n",
       "3     0.061458  \n",
       "4     0.215153  \n",
       "...        ...  \n",
       "4995  0.147968  \n",
       "4996  0.186637  \n",
       "4997  0.194361  \n",
       "4998 -0.253700  \n",
       "4999 -0.015309  \n",
       "\n",
       "[5000 rows x 30 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting 70% of the datasets \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [4327 4175  282 ... 1837 1914 3282] Test: [1930 2723 1968 ... 2956 3674 3090]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    data_Xtrain, data_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    data_ytrain, data_ytest = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Label Distributions: [Fraud Not-Fraud]\n",
      "Train: \n",
      "[0.90171429 0.09828571]\n",
      "[3156  344]\n",
      "Test: \n",
      "[0.90133333 0.09866667]\n",
      "[1352  148]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>0.057837</td>\n",
       "      <td>0.740863</td>\n",
       "      <td>1.347239</td>\n",
       "      <td>-0.468724</td>\n",
       "      <td>0.659279</td>\n",
       "      <td>-0.494070</td>\n",
       "      <td>-0.960005</td>\n",
       "      <td>-0.556445</td>\n",
       "      <td>-0.771329</td>\n",
       "      <td>-0.084322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>-0.051700</td>\n",
       "      <td>-0.223821</td>\n",
       "      <td>-0.554318</td>\n",
       "      <td>0.179944</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>0.096299</td>\n",
       "      <td>-0.487627</td>\n",
       "      <td>-0.008361</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.102669</td>\n",
       "      <td>0.736033</td>\n",
       "      <td>1.421832</td>\n",
       "      <td>-0.492554</td>\n",
       "      <td>-0.023678</td>\n",
       "      <td>-1.024934</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.785589</td>\n",
       "      <td>-0.831478</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982620</td>\n",
       "      <td>0.092759</td>\n",
       "      <td>-0.271269</td>\n",
       "      <td>-0.669532</td>\n",
       "      <td>-0.062000</td>\n",
       "      <td>-1.794172</td>\n",
       "      <td>0.333973</td>\n",
       "      <td>-0.401932</td>\n",
       "      <td>-0.007592</td>\n",
       "      <td>-0.012509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1.095140</td>\n",
       "      <td>-0.822083</td>\n",
       "      <td>-1.269461</td>\n",
       "      <td>-1.027153</td>\n",
       "      <td>2.261373</td>\n",
       "      <td>-0.504356</td>\n",
       "      <td>0.693421</td>\n",
       "      <td>1.237761</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.091919</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.463993</td>\n",
       "      <td>-0.527727</td>\n",
       "      <td>-0.131777</td>\n",
       "      <td>0.697266</td>\n",
       "      <td>0.288501</td>\n",
       "      <td>-0.644487</td>\n",
       "      <td>0.027409</td>\n",
       "      <td>-0.329714</td>\n",
       "      <td>-0.291795</td>\n",
       "      <td>-0.381556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.171116</td>\n",
       "      <td>-0.231588</td>\n",
       "      <td>-0.693979</td>\n",
       "      <td>0.863780</td>\n",
       "      <td>1.782080</td>\n",
       "      <td>-0.621203</td>\n",
       "      <td>-0.034457</td>\n",
       "      <td>-0.556248</td>\n",
       "      <td>0.704357</td>\n",
       "      <td>0.061961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121181</td>\n",
       "      <td>0.081262</td>\n",
       "      <td>-0.124982</td>\n",
       "      <td>-0.122718</td>\n",
       "      <td>-0.158336</td>\n",
       "      <td>0.590370</td>\n",
       "      <td>-0.014215</td>\n",
       "      <td>0.344616</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.157360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>0.411533</td>\n",
       "      <td>0.057732</td>\n",
       "      <td>-0.528280</td>\n",
       "      <td>1.297138</td>\n",
       "      <td>1.497918</td>\n",
       "      <td>1.128073</td>\n",
       "      <td>1.015072</td>\n",
       "      <td>0.182683</td>\n",
       "      <td>1.317492</td>\n",
       "      <td>-0.331670</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.154711</td>\n",
       "      <td>-0.009357</td>\n",
       "      <td>-0.056918</td>\n",
       "      <td>-0.080751</td>\n",
       "      <td>-0.286461</td>\n",
       "      <td>-0.652466</td>\n",
       "      <td>-0.042145</td>\n",
       "      <td>-0.245139</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>-0.102000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "4327       0.057837     0.740863  1.347239 -0.468724  0.659279 -0.494070   \n",
       "4175       0.102669     0.736033  1.421832 -0.492554 -0.023678 -1.024934   \n",
       "282        1.095140    -0.822083 -1.269461 -1.027153  2.261373 -0.504356   \n",
       "1999      -0.171116    -0.231588 -0.693979  0.863780  1.782080 -0.621203   \n",
       "2662       0.411533     0.057732 -0.528280  1.297138  1.497918  1.128073   \n",
       "\n",
       "            V5        V6        V7        V8  ...       V19       V20  \\\n",
       "4327 -0.960005 -0.556445 -0.771329 -0.084322  ...  0.049968 -0.051700   \n",
       "4175  0.038346  0.785589 -0.831478  0.108989  ...  0.982620  0.092759   \n",
       "282   0.693421  1.237761  0.056065  0.091919  ... -3.463993 -0.527727   \n",
       "1999 -0.034457 -0.556248  0.704357  0.061961  ...  0.121181  0.081262   \n",
       "2662  1.015072  0.182683  1.317492 -0.331670  ... -1.154711 -0.009357   \n",
       "\n",
       "           V21       V22       V23       V24       V25       V26       V27  \\\n",
       "4327 -0.223821 -0.554318  0.179944 -0.040405  0.096299 -0.487627 -0.008361   \n",
       "4175 -0.271269 -0.669532 -0.062000 -1.794172  0.333973 -0.401932 -0.007592   \n",
       "282  -0.131777  0.697266  0.288501 -0.644487  0.027409 -0.329714 -0.291795   \n",
       "1999 -0.124982 -0.122718 -0.158336  0.590370 -0.014215  0.344616  0.284906   \n",
       "2662 -0.056918 -0.080751 -0.286461 -0.652466 -0.042145 -0.245139 -0.188071   \n",
       "\n",
       "           V28  \n",
       "4327  0.012329  \n",
       "4175 -0.012509  \n",
       "282  -0.381556  \n",
       "1999  0.157360  \n",
       "2662 -0.102000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check the distribution of data\n",
    "train_unique_label, train_counts_label = np.unique(data_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(data_ytest, return_counts=True)\n",
    "print('-' * 50)\n",
    "\n",
    "print('Label Distributions: [Fraud Not-Fraud]')\n",
    "print('Train: ')\n",
    "print(train_counts_label/ len(data_ytrain))\n",
    "print(train_counts_label)\n",
    "print('Test: ')\n",
    "print(test_counts_label/ len(data_ytest))\n",
    "print(test_counts_label)\n",
    "\n",
    "data_Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3500 entries, 4327 to 3282\n",
      "Data columns (total 30 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   scaled_amount  3500 non-null   float64\n",
      " 1   scaled_time    3500 non-null   float64\n",
      " 2   V1             3500 non-null   float64\n",
      " 3   V2             3500 non-null   float64\n",
      " 4   V3             3500 non-null   float64\n",
      " 5   V4             3500 non-null   float64\n",
      " 6   V5             3500 non-null   float64\n",
      " 7   V6             3500 non-null   float64\n",
      " 8   V7             3500 non-null   float64\n",
      " 9   V8             3500 non-null   float64\n",
      " 10  V9             3500 non-null   float64\n",
      " 11  V10            3500 non-null   float64\n",
      " 12  V11            3500 non-null   float64\n",
      " 13  V12            3500 non-null   float64\n",
      " 14  V13            3500 non-null   float64\n",
      " 15  V14            3500 non-null   float64\n",
      " 16  V15            3500 non-null   float64\n",
      " 17  V16            3500 non-null   float64\n",
      " 18  V17            3500 non-null   float64\n",
      " 19  V18            3500 non-null   float64\n",
      " 20  V19            3500 non-null   float64\n",
      " 21  V20            3500 non-null   float64\n",
      " 22  V21            3500 non-null   float64\n",
      " 23  V22            3500 non-null   float64\n",
      " 24  V23            3500 non-null   float64\n",
      " 25  V24            3500 non-null   float64\n",
      " 26  V25            3500 non-null   float64\n",
      " 27  V26            3500 non-null   float64\n",
      " 28  V27            3500 non-null   float64\n",
      " 29  V28            3500 non-null   float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 847.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_Xtrain.info())\n",
    "#print(data_ytrain.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#to prints and plots the confusion matrix \n",
    "#Normilization is done by setting normalize=True\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model phase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the data loaded into original_Xtrain, original_ytrain, original_Xtest, and original_ytest\n",
    "\n",
    "# Initialize StratifiedShuffleSplit for cross-validation\n",
    "sss = StratifiedShuffleSplit(n_splits=7, test_size=0.0015, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=30))  # Update input_dim to 29\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = RobustScaler()\n",
    "history_list = []\n",
    "\n",
    "# Initialize an array to store accuracy scores for each fold\n",
    "accuracy_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [4778, 4882, 4730, 4674, 4638, 4593, 4989, 4602, 4579, 4978, 4532, 4773, 4818, 4831, 4572, 4865, 4595, 4782, 4873, 4817, 4994, 4557, 4954, 4528, 4993, 4560, 4533, 4512, 4768, 4806, 4608, 4735, 4641, 4620, 4683, 4615, 4543, 4686, 4957, 4791, 4981, 4738, 4905, 4871, 4943, 4976, 4767, 4892, 4656, 4740, 4896, 4797, 4654, 4960, 4836, 4977, 4710, 4794, 4834, 4644, 4605, 4956, 4633, 4558, 4519, 4975, 4544, 4531, 4852, 4990, 4765, 4965, 4811, 4915, 4545, 4585, 4945, 4712, 4691, 4665, 4800, 4671, 4606, 4627, 4660, 4723, 4621, 4726, 4813, 4707, 4944, 4573, 4525, 4521, 4574, 4759, 4844, 4898, 4648, 4810, ...]\n",
      "\n",
      "[686 rows x 0 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\iraha\\OneDrive - IMAM ABDULRAHMAN BIN FAISAL UNIVERSITY\\Desktop\\AI Levels\\Senior Level 1\\Projects\\DL Project\\CreditCardFraudDetection\\SubGroup2.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(X_train_resampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Scale the data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m X_train_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X_train_resampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m X_val_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1533\u001b[0m, in \u001b[0;36mRobustScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the median and quantiles to be used for scaling.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \n\u001b[0;32m   1517\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39m# at fit, convert sparse matrices to csc for optimized computation of\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[39m# the quantiles\u001b[39;00m\n\u001b[1;32m-> 1533\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1534\u001b[0m     X,\n\u001b[0;32m   1535\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1536\u001b[0m     dtype\u001b[39m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1537\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1538\u001b[0m )\n\u001b[0;32m   1540\u001b[0m q_min, q_max \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantile_range\n\u001b[0;32m   1541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m q_min \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m q_max \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:797\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    793\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    794\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    795\u001b[0m )\n\u001b[0;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 797\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mresult_type(\u001b[39m*\u001b[39mdtypes_orig)\n\u001b[0;32m    798\u001b[0m \u001b[39melif\u001b[39;00m pandas_requires_conversion \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(d \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    799\u001b[0m     \u001b[39m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    800\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for train_val_index, test_val_index in sss.split(data_Xtrain, data_ytrain):\n",
    "    X_train, X_val = data_Xtrain.iloc[train_val_index], data_Xtrain.iloc[test_val_index]\n",
    "    y_train, y_val = data_ytrain.iloc[train_val_index], data_ytrain.iloc[test_val_index]\n",
    "\n",
    "    #train_val_data =  pd.DataFrame(X_train)\n",
    "   \n",
    "    train_val_data = train_val_data.assign(Class=y_train)\n",
    "    #train_val_data=pd.DataFrame(train_val_data)\n",
    "\n",
    "    fraud_df = train_val_data.loc[train_val_data['Class'] == 1]\n",
    "    notfraud_df = train_val_data.loc[train_val_data['Class'] == 0].sample(len(fraud_df))\n",
    "\n",
    "    print(f'Relation fraud non-fraud in the CV set: {len(fraud_df)} x {len(non_fraud_df)}')\n",
    "\n",
    "    print(f'Test training relationship {len(notfraud_df)+len(fraud_df)} x {len(y_val)}')\n",
    "    print(f'Test training relationship {(len(notfraud_df)+len(fraud_df))/(len(non_fraud_df)+len(fraud_df)+len(y_val))} x {len(y_val)/(len(non_fraud_df)+len(fraud_df)+len(y_val))}')\n",
    "    \n",
    "        \n",
    "        \n",
    "    X_train_resampled = pd.concat([fraud_df, notfraud_df]).drop('Class', axis=1)\n",
    "    y_train_resampled = pd.concat([fraud_df, notfraud_df])['Class']\n",
    "    \n",
    "    print(X_train_resampled)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Fit the model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    history = model.fit(x=X_train_scaled, y=y_train_resampled, epochs=1000, validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=[early_stop])\n",
    "\n",
    "    history_list.append(history.history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_scaled, y_val)\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "print(f\"Average Accuracy Neural Network: {np.mean(accuracy_scores)}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_scaled = scaler.transform(data_Xtest)\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(data_ytest, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_ytest, y_pred))\n",
    "print(\"Recall: \", recall_score(data_ytest, y_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0, 1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4778       5.806468    36.315004 -1.430864 -0.802529  1.123320  0.389760   \n",
      "1845      -0.222793    -0.281638  1.192420  0.076970  0.583453  0.665919   \n",
      "2236      -0.234600    -0.133246  1.241908  0.205299  0.398826  0.424773   \n",
      "2865      -0.140828     0.156514 -0.436030  1.635475  1.647855  2.468624   \n",
      "1564       4.482033    -0.371200 -2.749525 -3.300879  1.933082  0.312397   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "3024       0.331622     0.221491 -1.335640  0.093758  1.716347  0.321310   \n",
      "4237       0.020363     0.738229  1.451138 -0.601564 -0.886202 -1.637269   \n",
      "1419       6.026694    -0.428274  0.483432 -1.533762  0.618801 -0.491250   \n",
      "1656      -0.008556    -0.345736 -3.835659 -1.955510 -0.404067  0.438873   \n",
      "648       -0.085558    -0.694326  1.130024 -0.060944  0.662074  1.033990   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "4778 -0.281214 -0.055123  1.326232  0.195700  ...  0.634184  0.325575   \n",
      "1845 -0.617114 -0.753429 -0.133273  0.016383  ... -0.252928 -0.229348   \n",
      "2236 -0.243974 -0.473666 -0.036665 -0.047231  ... -0.060969 -0.225866   \n",
      "2865  0.625894  0.347932  0.825010 -0.188491  ...  0.558709 -0.321424   \n",
      "1564  1.442330 -1.421397 -0.535266  0.097426  ...  0.784980 -0.211812   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "3024  0.597336 -1.024550  0.340861 -0.280405  ... -0.197070 -0.308835   \n",
      "4237  1.483689  3.265473 -1.204579  0.717045  ...  0.169572 -0.293726   \n",
      "1419 -1.570267 -0.450331 -0.171666 -0.039912  ...  0.676298  0.432930   \n",
      "1656  1.787822 -1.066112 -0.275514  0.221039  ... -1.180631 -0.661170   \n",
      "648  -0.441560  0.310187 -0.559948  0.349136  ... -0.258810  0.195852   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "4778  0.014002  0.844946  0.114963  0.156365 -0.619437 -0.120351  0.035594   \n",
      "1845 -0.720419  0.228258  0.344679  0.038809  0.110178 -0.024190  0.015007   \n",
      "2236 -0.678312  0.081198 -0.002696  0.237988  0.097226 -0.032047  0.005763   \n",
      "2865 -0.544905 -0.144442 -0.474564 -0.241057 -0.015167  0.330356  0.035417   \n",
      "1564 -1.222846  0.948554  0.196215  0.647151 -0.652061 -0.347416 -0.176205   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3024 -0.355747 -0.393202  0.419444 -0.127249  0.201127  0.049004 -0.085552   \n",
      "4237 -0.884656  0.103960  0.918699  0.366493 -0.505633 -0.015061  0.010953   \n",
      "1419  0.696581 -0.451252  0.613066  0.459382 -0.602441  0.027400  0.090133   \n",
      "1656  0.285619  1.336996 -0.688062  0.784588  1.281218  0.224189 -0.883502   \n",
      "648   0.511310 -0.100457 -0.344298  0.420079 -0.213097  0.040658  0.010599   \n",
      "\n",
      "      Class  \n",
      "4778      1  \n",
      "1845      0  \n",
      "2236      0  \n",
      "2865      0  \n",
      "1564      0  \n",
      "...     ...  \n",
      "3024      0  \n",
      "4237      0  \n",
      "1419      0  \n",
      "1656      0  \n",
      "648       0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3        V4  \\\n",
      "4778       5.806468    36.315004  -1.430864 -0.802529   1.123320  0.389760   \n",
      "4882      -0.173340    55.822412   1.177824  2.487103  -5.330608  5.324547   \n",
      "4730       8.123032    28.570958  -1.824295  0.403327  -1.994122  2.756558   \n",
      "4674       1.454312    21.529799  -9.848776  7.365546 -12.898538  4.273323   \n",
      "4638       4.414956    17.264625 -14.970346  8.401421 -16.867238  8.252334   \n",
      "...             ...          ...        ...       ...        ...       ...   \n",
      "1450      -0.239562    -0.415103  -1.380849 -0.145625   0.126665 -2.162818   \n",
      "3566       0.937714     0.428713  -1.307922  1.117889   1.206546 -0.769958   \n",
      "171        1.632444    -0.861596  -1.276830 -0.903557   1.477105 -2.582626   \n",
      "2050       0.164442    -0.213149   1.153677  0.216433   0.553173  1.532029   \n",
      "2046      -0.171116    -0.214905  -1.361981  1.186318  -0.513394 -1.797613   \n",
      "\n",
      "             V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4778  -0.281214 -0.055123   1.326232  0.195700  ... -1.140376  0.634184   \n",
      "4882   1.150243 -1.281843  -1.171994  0.413778  ... -1.359844  0.156057   \n",
      "4730  -3.139064  0.408185  -1.209045  1.095634  ...  2.690808  1.503688   \n",
      "4674  -7.611991 -3.427045  -8.350808  6.863604  ...  0.554027  0.832398   \n",
      "4638 -13.565130 -2.782438 -14.263735  9.643419  ...  1.243517 -1.614660   \n",
      "...         ...       ...        ...       ...  ...       ...       ...   \n",
      "1450   0.521847 -1.741244   0.019423  0.381790  ...  1.011011  0.205208   \n",
      "3566   0.122376 -0.323248   1.210819  0.105585  ... -2.101730 -0.324048   \n",
      "171    0.635981 -0.866537   1.138067 -0.924605  ...  0.100918 -0.431083   \n",
      "2050  -0.504798 -0.829963   0.177455 -0.181344  ... -0.137654 -0.133181   \n",
      "2046   2.046239  3.397838  -0.215105  1.427061  ...  0.222344  0.282483   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4778  0.325575  0.014002  0.844946  0.114963  0.156365 -0.619437 -0.120351   \n",
      "4882  0.262325 -0.431790 -0.092088  0.145216  0.457788  0.167739  0.451243   \n",
      "4730  0.838760  0.341727  0.947506 -0.145493  0.049326  0.831065  0.332421   \n",
      "4674  0.931958 -0.874467 -0.192639 -0.035426  0.538665 -0.263934  1.134095   \n",
      "4638  2.714045 -0.101355 -0.439666  0.519514  0.789328  0.064357 -1.621386   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1450  0.189500  0.234031 -0.202100  0.041873 -0.080313 -0.137458  0.245276   \n",
      "3566  0.105867  0.436059 -0.240564  0.170701  0.076297 -0.732521 -0.354709   \n",
      "171  -0.482119 -0.182518 -0.614101  0.037650  0.793080 -0.724708 -0.616486   \n",
      "2050 -0.097674 -0.115690 -0.066010  0.734489  0.688098 -0.364597  0.021047   \n",
      "2046 -0.274026 -0.706508 -0.130367  1.026032  0.297645  0.404274  0.372825   \n",
      "\n",
      "           V28  \n",
      "4778  0.035594  \n",
      "4882  0.268421  \n",
      "4730  0.252713  \n",
      "4674  0.225973  \n",
      "4638 -1.104819  \n",
      "...        ...  \n",
      "1450 -0.024124  \n",
      "3566 -0.376536  \n",
      "171  -0.485985  \n",
      "2050  0.025677  \n",
      "2046  0.214961  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.6751e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.6357e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 5.0089e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5833e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.3074e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0850e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0463e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.8731e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6645e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.5551e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.5551e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4626       1.253080    17.179453 -8.426814  6.241659 -9.946470  8.199614   \n",
      "2863      -0.239562     0.156075 -0.322984  0.711644  0.896844 -2.152729   \n",
      "2459      -0.085729    -0.019537 -0.836395  1.222372  1.589788  0.005322   \n",
      "528        1.151437    -0.737789 -2.153491  0.104021  1.459532 -1.613284   \n",
      "1186       0.833333    -0.504226 -0.658874  0.994656  1.621280  3.371777   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "3515       0.171458     0.409834 -1.311789  1.778326  1.033075  2.767400   \n",
      "4265       0.172142     0.738668 -1.320585 -0.991043  0.633928 -2.481152   \n",
      "2653      -0.222793     0.051147  0.076641  0.840775  0.630554  0.433788   \n",
      "1717      -0.017283    -0.325102 -0.695350 -0.973853  2.452479 -0.676376   \n",
      "185        0.427789    -0.855888 -0.505728  0.666300  1.080993 -0.447630   \n",
      "\n",
      "            V5        V6         V7        V8  ...       V20       V21  \\\n",
      "4626 -8.213093 -2.522046 -11.643028  5.339500  ...  0.563869  2.427460   \n",
      "2863  0.429934 -1.215491   1.174949 -0.421007  ...  0.057245  0.029648   \n",
      "2459 -0.198704 -1.074656   0.716620 -0.235059  ...  0.058887 -0.153673   \n",
      "528  -1.071361  0.192161  -0.332748  0.462717  ...  0.227422 -0.101937   \n",
      "1186  0.151203 -0.184932   0.655278 -0.291056  ...  0.061760 -0.048308   \n",
      "...        ...       ...        ...       ...  ...       ...       ...   \n",
      "3515  0.368220  0.045334   0.705935  0.214069  ... -0.149862  0.122470   \n",
      "4265  3.081888  2.647272  -1.067603  0.906425  ...  0.492035 -0.146948   \n",
      "2653  0.015145  0.348943  -0.587464 -1.402942  ... -0.350245  1.316097   \n",
      "1717 -0.126656 -0.240814  -0.878516 -0.105118  ...  0.191317  0.453279   \n",
      "185   1.039490  0.184017   0.882797 -0.142340  ...  0.435055 -0.058792   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "4626  0.692667  0.020305  0.499809  0.467594  0.483162  1.195671  0.198294   \n",
      "2863  0.476991 -0.359698 -0.104292  0.145480 -0.809105  0.239337 -0.018368   \n",
      "2459 -0.495737  0.010515  0.708366 -0.256932  0.010587 -0.136191  0.136858   \n",
      "528   0.096735 -0.490325 -0.392662  0.608578 -0.338382 -0.352132 -0.422726   \n",
      "1186  0.266926 -0.173916  0.649605 -0.456715  0.169096 -0.194693  0.014633   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3515  0.311254 -0.082732  0.116752  0.415781  0.357548 -0.395629 -0.023097   \n",
      "4265 -0.909814  0.060099  0.899750  0.435219 -0.589154 -0.043979  0.073881   \n",
      "2653 -1.191293  0.051294 -0.378302  0.228462  0.246281  0.209029  0.242257   \n",
      "1717  1.585425 -0.303820  0.263144 -0.638176 -0.035985 -0.180979 -0.324253   \n",
      "185   0.022073 -0.226241 -0.986435 -0.065937  0.259685  0.150573 -0.042581   \n",
      "\n",
      "      Class  \n",
      "4626      1  \n",
      "2863      0  \n",
      "2459      0  \n",
      "528       0  \n",
      "1186      0  \n",
      "...     ...  \n",
      "3515      0  \n",
      "4265      0  \n",
      "2653      0  \n",
      "1717      0  \n",
      "185       0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1         V2         V3         V4  \\\n",
      "4626       1.253080    17.179453  -8.426814   6.241659  -9.946470   8.199614   \n",
      "4777      -0.046030    36.273296  -4.868108   1.264420  -5.167885   3.193648   \n",
      "4521      -0.239562     2.641861  -1.783229   3.402794  -3.822742   2.625368   \n",
      "4836      -0.108830    41.729119 -17.976266  12.864989 -19.575066  11.345120   \n",
      "4849      -0.217659    44.109099  -1.456876   3.740306  -7.404518   7.440964   \n",
      "...             ...          ...        ...        ...        ...        ...   \n",
      "3361      -0.239562     0.367687  -0.300381   0.570978   0.716708  -2.214918   \n",
      "3441      -0.088809     0.379980  -5.151104   2.977071  -0.859917  -0.739685   \n",
      "1537       2.481177    -0.379541   0.952131  -0.660004  -0.351173  -1.408060   \n",
      "3541       0.940965     0.419054  -1.370946   0.593587   1.842636   0.223817   \n",
      "3683       1.966119     0.470859   0.922537  -0.517835   0.832344   0.913536   \n",
      "\n",
      "             V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4626  -8.213093 -2.522046 -11.643028  5.339500  ...  3.204309  0.563869   \n",
      "4777  -3.045621 -2.096166  -6.445610  2.422536  ...  1.385610  0.667310   \n",
      "4521  -1.976415 -2.731689  -3.430559  1.413204  ... -0.265422  0.364089   \n",
      "4836 -13.998646 -0.354900 -23.783470 -4.872353  ...  1.661029  2.419921   \n",
      "4849  -1.549878 -1.661697  -5.757213  1.615011  ...  0.065317  0.529557   \n",
      "...         ...       ...        ...       ...  ...       ...       ...   \n",
      "3361   0.433500 -1.064170   1.044465 -0.296050  ...  0.225710 -0.048111   \n",
      "3441   0.573801  2.065386  -1.264569 -4.303179  ... -1.662652 -0.660260   \n",
      "1537  -0.486406 -0.962104   0.341565 -0.203722  ...  0.624917  0.224722   \n",
      "3541  -0.502081 -0.092788   0.231925  0.210382  ... -0.445934  0.301482   \n",
      "3683  -0.826167  0.111915  -0.354734  0.082614  ... -0.463821  0.140578   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4626  2.427460  0.692667  0.020305  0.499809  0.467594  0.483162  1.195671   \n",
      "4777  1.269205  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274   \n",
      "4521  0.454032 -0.577526  0.045967  0.461700  0.044146  0.305704  0.530981   \n",
      "4836 -4.300432  2.865772  1.489302  0.386039 -0.232392  0.413827 -3.255981   \n",
      "4849  0.957897  0.145339 -0.044704 -0.544962 -0.757757 -0.005352  0.318152   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3361 -0.013581  0.265651 -0.379911 -0.494418  0.129384 -0.781942  0.227018   \n",
      "3441  3.884687 -1.893838  1.277402 -1.635821  0.344465  0.319386  1.223308   \n",
      "1537  0.153141  0.237123 -0.210545  0.298511  0.529161 -0.004023 -0.034051   \n",
      "3541  0.088592  0.794873 -0.083819  0.479359 -0.093705  0.475409  0.582144   \n",
      "3683  0.167155  0.493199 -0.187079  0.174856  0.390691  0.524809  0.001312   \n",
      "\n",
      "           V28  \n",
      "4626  0.198294  \n",
      "4777 -0.127944  \n",
      "4521  0.243746  \n",
      "4836 -0.538963  \n",
      "4849 -0.323554  \n",
      "...        ...  \n",
      "3361 -0.025715  \n",
      "3441  0.940238  \n",
      "1537  0.021217  \n",
      "3541  0.436372  \n",
      "3683  0.037852  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.2611e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9674e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.0281e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.9746e-04 - accuracy: 1.0000 - val_loss: 2.9976e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.4698e-04 - accuracy: 1.0000 - val_loss: 3.1248e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.9130e-04 - accuracy: 1.0000 - val_loss: 2.9960e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.4743e-04 - accuracy: 1.0000 - val_loss: 2.8923e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.1435e-04 - accuracy: 1.0000 - val_loss: 2.8421e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 7.8664e-04 - accuracy: 1.0000 - val_loss: 2.7896e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 7.4893e-04 - accuracy: 1.0000 - val_loss: 2.7267e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7267e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "3362      -0.239562     0.368126 -0.540909  0.485743  0.646909 -2.256652   \n",
      "2305      -0.137064    -0.097245  1.204461 -0.084453  0.479036  0.703425   \n",
      "970        0.176249    -0.586763  1.444837 -1.245312  0.776082 -1.491727   \n",
      "1270       0.002225    -0.480079 -0.595254  0.993109  1.788433  2.066058   \n",
      "1615      -0.239562    -0.358468 -2.594704  0.792116 -0.749193 -2.073082   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "1226       0.237851    -0.494128  1.130882 -0.306948  0.998749  0.225915   \n",
      "4848       1.522930    44.085830 -2.296987  4.064043 -5.957706  4.680008   \n",
      "1368      -0.239733    -0.442322  1.313171  0.078188 -1.291722 -0.483959   \n",
      "4474       0.351300     0.746570  1.094368  0.148772  1.171742  1.309366   \n",
      "4207       1.364648     0.736911  1.299369 -0.852354 -0.974292 -1.573454   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "3362  0.671964 -0.626104  0.942382 -0.196446  ... -0.100149  0.027194   \n",
      "2305 -0.456606 -0.096802 -0.268881  0.089622  ... -0.128189 -0.063122   \n",
      "970  -1.765425 -0.270934 -1.443537  0.108861  ... -0.333260 -0.211910   \n",
      "1270  0.689541  2.173362 -0.055776  0.626694  ... -0.028686 -0.273816   \n",
      "1615 -0.938225 -1.819945 -0.102909  1.103437  ...  0.070067  0.173200   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "1226 -0.544978  0.872664 -0.938950  0.372366  ...  0.024383  0.261644   \n",
      "4848 -2.080938 -1.463272 -4.490847  1.029246  ...  1.264954  1.089084   \n",
      "1368  2.140233  3.181208 -0.506359  0.805117  ...  0.030718 -0.343276   \n",
      "4474 -0.533612  0.028375 -0.505245 -0.014002  ... -0.072348  0.103551   \n",
      "4207  1.406209  3.293744 -1.100331  0.696654  ...  0.329556 -0.246124   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "3362  0.393594 -0.256605 -0.806563  0.069317 -0.761794  0.119072 -0.041058   \n",
      "2305  0.010919 -0.136416  0.065784  0.585370  0.442097 -0.022990 -0.003397   \n",
      "970  -0.336079  0.071196 -0.055183  0.134178 -0.291773  0.043139  0.018885   \n",
      "1270 -0.394840 -0.301300 -1.411518  0.045571 -0.019724  0.012914  0.067788   \n",
      "1615  0.377772  0.148204  0.587936  0.071497 -0.153507  0.262471 -0.081060   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1226  0.792110 -0.145284 -0.763391  0.212113  0.638136  0.024789  0.013678   \n",
      "4848  0.975398 -0.625530 -0.535181  0.247435  0.160400  0.969582  0.335041   \n",
      "1368 -1.147981  0.096151  0.946070  0.370076  0.110731 -0.021650  0.023466   \n",
      "4474  0.625461 -0.084649  0.061918  0.409214 -0.276609  0.047594  0.033268   \n",
      "4207 -0.926119  0.034717  0.922730  0.337647 -0.514011 -0.029904  0.024632   \n",
      "\n",
      "      Class  \n",
      "3362      0  \n",
      "2305      0  \n",
      "970       0  \n",
      "1270      0  \n",
      "1615      0  \n",
      "...     ...  \n",
      "1226      0  \n",
      "4848      1  \n",
      "1368      0  \n",
      "4474      0  \n",
      "4207      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3         V4  \\\n",
      "4660      -0.239562    19.083964  -1.519244  2.308492  -1.503599   2.064101   \n",
      "4745      -0.243669    30.185710   0.196707  1.189757   0.704882   2.891388   \n",
      "4675       1.454312    21.539897 -10.527304  7.639745 -13.443115   4.303403   \n",
      "4709       5.205339    25.333992  -8.257111 -4.814461  -5.365307   1.204230   \n",
      "4804      -0.256502    40.286028  -3.765680  5.890735 -10.202268  10.259036   \n",
      "...             ...          ...        ...       ...        ...        ...   \n",
      "1448      -0.077687    -0.416420  -1.102491  1.355435   2.118981   0.242654   \n",
      "83         1.269165    -0.886621  -1.198968 -1.474100   1.840326  -4.515824   \n",
      "2093      -0.239562    -0.201295  -2.962376  2.607363  -0.067883  -0.889316   \n",
      "2720       0.924025     0.084513  -1.266171 -0.321459   1.784605  -2.404879   \n",
      "2473      -0.241444    -0.014707  -4.454186 -3.581156  -0.522735   2.216885   \n",
      "\n",
      "            V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4660 -1.000845 -1.016897  -2.059731 -0.275166  ...  1.160120  0.175019   \n",
      "4745  0.045555  1.245730  -1.198714 -2.421616  ...  0.329827  0.646616   \n",
      "4675 -8.048210 -3.466997  -8.643193  7.284105  ...  0.511762  0.847085   \n",
      "4709 -3.347420 -1.331601  -1.967893  1.295438  ...  0.105879 -1.233987   \n",
      "4804 -5.611448 -3.235376 -10.632683  3.272716  ...  2.500827  1.194137   \n",
      "...        ...       ...        ...       ...  ...       ...       ...   \n",
      "1448  0.685721  0.468232   1.130935 -0.675945  ...  1.119734  0.439737   \n",
      "83    0.327567 -0.174469   0.959726 -1.026456  ... -1.512022 -0.769854   \n",
      "2093 -1.360313 -0.983393  -0.534070  1.473261  ... -0.202505  0.420581   \n",
      "2720 -1.251838 -0.348121  -0.675538  0.489034  ... -1.356222 -0.015154   \n",
      "2473  6.077260 -5.500163  -1.928049 -0.207121  ... -1.434199  0.155928   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4660  1.307871  0.102826 -0.017746  0.149696 -0.096602 -0.369115 -0.019244   \n",
      "4745 -1.328132  0.189311 -0.005524 -0.814708  0.400924  0.286281  0.135215   \n",
      "4675  0.937416 -0.931178 -0.235697 -0.031393  0.591558 -0.263516  1.108897   \n",
      "4709  0.436390 -0.077553 -3.091624 -0.390201 -0.288689 -0.340004  0.039819   \n",
      "4804  2.245606  0.546321  0.381853  0.382025 -0.821036  0.394355  1.412961   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1448 -0.310349 -0.284566 -0.399066 -0.519518  0.134754 -0.644481 -1.273598   \n",
      "83   -0.453169  0.334614 -0.364541 -0.310186 -0.302599 -1.243924 -1.123457   \n",
      "2093 -0.185342 -0.455737  0.160595  0.506340  0.078372  0.299425  0.544370   \n",
      "2720  0.085012  0.446617 -0.090068  0.095560  0.434890 -0.147216  0.196593   \n",
      "2473  0.036795 -0.215103  0.096331  0.669872 -0.168050 -0.595656  0.045526   \n",
      "\n",
      "           V28  \n",
      "4660 -0.208319  \n",
      "4745  0.257315  \n",
      "4675  0.219021  \n",
      "4709 -1.007900  \n",
      "4804  0.782407  \n",
      "...        ...  \n",
      "1448 -1.057595  \n",
      "83   -0.734351  \n",
      "2093  0.349456  \n",
      "2720  0.023946  \n",
      "2473  0.851741  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.5942e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 8.9903e-04 - accuracy: 1.0000 - val_loss: 3.5621e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 7.9849e-04 - accuracy: 1.0000 - val_loss: 3.5862e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 7.4796e-04 - accuracy: 1.0000 - val_loss: 3.5803e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 7.0281e-04 - accuracy: 1.0000 - val_loss: 3.4087e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 6.6988e-04 - accuracy: 1.0000 - val_loss: 3.1974e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 6.4297e-04 - accuracy: 1.0000 - val_loss: 3.1883e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 6.1323e-04 - accuracy: 1.0000 - val_loss: 3.1874e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 5.9812e-04 - accuracy: 1.0000 - val_loss: 3.0841e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 5.6961e-04 - accuracy: 1.0000 - val_loss: 2.9550e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9550e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "3165       0.910849     0.293930 -1.032672  0.288381  2.063876  0.353579   \n",
      "1880      -0.195243    -0.273296 -0.669051  1.137969  1.169320 -0.147124   \n",
      "4649      -0.239562    17.637800 -2.524012  2.098152 -4.946075  6.456588   \n",
      "795        2.048255    -0.644276  0.888448 -0.629868  1.344220  0.715795   \n",
      "974       -0.085729    -0.585446  1.182742  0.235351  0.389091  0.638299   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "4479      -0.256674     0.750960  1.106262  0.398131  1.815655  2.990708   \n",
      "2704      -0.239562     0.076611 -1.363919  0.247680  0.909595  1.127503   \n",
      "4335      -0.074949     0.741302  1.017648 -0.043059  0.684992  0.913154   \n",
      "1494       2.446954    -0.398859  0.935047 -0.584762  0.196066  0.447476   \n",
      "4328       0.316222     0.740863  1.417736 -0.656760 -0.905589 -1.623224   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "3165 -0.298921 -0.365203  0.400751  0.095128  ... -0.209219  0.192060   \n",
      "1880  0.196541 -0.234215  0.461928  0.258223  ...  0.176081 -0.223611   \n",
      "4649  3.173921 -3.058806 -0.184710 -0.390420  ... -0.162797  0.027935   \n",
      "795  -1.449115 -0.188218 -0.766771  0.206372  ...  0.155566  0.359282   \n",
      "974  -0.479632 -0.877410 -0.045612 -0.034499  ... -0.101705 -0.225136   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "4479 -0.865004  0.009125 -0.626067  0.048107  ... -0.202821 -0.212720   \n",
      "2704  1.555518 -0.807158  1.390296 -1.017523  ... -0.371190 -0.357274   \n",
      "4335  0.133533  1.170727 -0.460130  0.378469  ... -0.324884 -0.242925   \n",
      "1494 -0.601640 -0.236399  0.000171  0.014678  ...  0.182982 -0.126611   \n",
      "4328  1.466636  3.271695 -1.181635  0.712557  ...  0.204782 -0.283250   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "3165  0.599978 -0.164235  0.386969  0.151719 -0.396472  0.003126  0.166778   \n",
      "1880 -0.536028  0.014242 -0.053043 -0.233447  0.091225  0.364371  0.142858   \n",
      "4649  0.220366  0.976348 -0.290539  1.161002  0.663954  0.456023 -0.405682   \n",
      "795   0.736915 -0.127126  0.573479  0.085966  0.429654 -0.009815  0.045354   \n",
      "974  -0.738237  0.142697  0.477843  0.133592  0.065680 -0.032476  0.022722   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4479 -0.199560  0.094663  0.646047  0.238918 -0.102657  0.018308  0.033028   \n",
      "2704  0.483963  0.470293 -0.082912 -0.230531 -0.388476 -0.439026 -0.779125   \n",
      "4335 -0.144215  0.192951 -0.674396  0.059524  0.339840  0.019932 -0.005076   \n",
      "1494 -0.588298 -0.156607  0.053381  0.343463  0.490779 -0.076862  0.020418   \n",
      "4328 -0.893782  0.088720  0.919586  0.360144 -0.507477 -0.018327  0.013964   \n",
      "\n",
      "      Class  \n",
      "3165      0  \n",
      "1880      0  \n",
      "4649      1  \n",
      "795       0  \n",
      "974       0  \n",
      "...     ...  \n",
      "4479      0  \n",
      "2704      0  \n",
      "4335      0  \n",
      "1494      0  \n",
      "4328      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4649      -0.239562    17.637800 -2.524012  2.098152 -4.946075  6.456588   \n",
      "4768       0.770021    33.652727 -4.312479  1.886476 -2.338634 -0.475243   \n",
      "4781      -0.256674    36.532763 -7.030308  3.421991 -9.525072  5.270891   \n",
      "4517      -0.239562     2.431127  0.725646  2.300894 -5.329976  4.007683   \n",
      "4783       4.071184    36.532763 -6.713407  3.921104 -9.746678  5.148263   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "2964      -0.243669     0.191636  1.335053  0.331464 -2.057763 -0.346175   \n",
      "3168       0.168549     0.295248  1.110541  0.212428  0.537610  1.476218   \n",
      "680        0.369781    -0.684228 -0.878340  0.489031  2.914341  0.078154   \n",
      "1385       2.446954    -0.437054 -2.095310  0.372403  0.186651 -0.440886   \n",
      "605       -0.205510    -0.708374  1.294157  1.132084 -2.148605  1.230031   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V19       V20  \\\n",
      "4649  3.173921 -3.058806 -0.184710 -0.390420  ... -1.063028 -0.162797   \n",
      "4768 -1.185444 -2.112079 -2.122793  0.272565  ... -0.009534 -0.718706   \n",
      "4781 -4.024630 -2.865682 -6.989195  3.791551  ...  0.876019  0.545698   \n",
      "4517 -1.730411 -1.732193 -3.968593  1.063728  ...  2.250123  0.504646   \n",
      "4783 -5.151563 -2.099389 -5.937767  3.578780  ...  0.897402  0.135711   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "2964  2.583234  2.854102 -0.187547  0.685154  ...  0.049350  0.045217   \n",
      "3168 -0.262395 -0.344631  0.142262 -0.078189  ... -0.500202 -0.124489   \n",
      "680  -0.423732  0.090021  0.289963  0.085451  ...  0.385202  0.363765   \n",
      "1385 -0.609495 -0.073792  0.923225  0.323543  ...  0.276185 -0.463425   \n",
      "605   1.447894 -0.767676  0.575635 -0.094475  ... -0.038914 -0.046194   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4649  0.027935  0.220366  0.976348 -0.290539  1.161002  0.663954  0.456023   \n",
      "4768  0.550541 -0.067870 -1.114692  0.269069 -0.020572 -0.963489 -0.918888   \n",
      "4781  1.103398 -0.541855  0.036943 -0.355519  0.353634  1.042458  1.359516   \n",
      "4517  0.589669  0.109541  0.601045 -0.364700 -1.843078  0.351909  0.594550   \n",
      "4783  0.954272 -0.451086  0.127214 -0.339450  0.394096  1.075295  1.649906   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2964 -0.191820 -0.650118 -0.114069  0.915936  0.730073  0.383879 -0.031902   \n",
      "3168 -0.062755  0.049586 -0.044224  0.440124  0.636320 -0.336996  0.041561   \n",
      "680  -0.037353  0.236372 -0.197062  0.584250  0.190462  0.305561  0.122975   \n",
      "1385 -0.209954 -0.373133  0.178694 -0.383664  0.014457  0.334662 -0.639994   \n",
      "605  -0.257463 -0.726440 -0.374828 -1.358994  0.968892 -0.225584  0.027084   \n",
      "\n",
      "           V28  \n",
      "4649 -0.405682  \n",
      "4768  0.001454  \n",
      "4781 -0.272188  \n",
      "4517  0.099372  \n",
      "4783 -0.394905  \n",
      "...        ...  \n",
      "2964  0.029849  \n",
      "3168  0.023643  \n",
      "680  -0.088435  \n",
      "1385 -0.252820  \n",
      "605   0.071322  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.1033e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 9.0313e-04 - accuracy: 1.0000 - val_loss: 9.0541e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 8.2249e-04 - accuracy: 1.0000 - val_loss: 9.1677e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 7.4681e-04 - accuracy: 1.0000 - val_loss: 1.0072e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 7.0695e-04 - accuracy: 1.0000 - val_loss: 1.0358e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 6.6499e-04 - accuracy: 1.0000 - val_loss: 1.0368e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 6.4412e-04 - accuracy: 1.0000 - val_loss: 1.0354e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 5.9759e-04 - accuracy: 1.0000 - val_loss: 1.0086e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 5.9120e-04 - accuracy: 1.0000 - val_loss: 1.0034e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 5.6266e-04 - accuracy: 1.0000 - val_loss: 1.0289e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0289e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "824        0.729637    -0.632861 -1.058931  0.114078  1.485337  0.534704   \n",
      "4777      -0.046030    36.273296 -4.868108  1.264420 -5.167885  3.193648   \n",
      "340       -0.154689    -0.800132 -1.337375  1.109391  1.396223  0.038718   \n",
      "4933      -0.229466    64.089782  1.261324  2.726800 -5.435019  5.342759   \n",
      "742        0.045859    -0.664911 -0.439880  0.630451  1.942591 -0.798747   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "955        2.820671    -0.592471 -2.240566  0.466023  1.566136  1.158490   \n",
      "2856      -0.239562     0.154758 -0.323085  0.701661  0.893979 -2.151390   \n",
      "4289       2.277379     0.739546  1.196322 -1.022632 -1.034102 -1.530126   \n",
      "4059       0.544148     0.723741  1.026189 -0.074026  1.497202  1.509733   \n",
      "1563       0.151951    -0.371200 -1.113706 -0.290950  1.117998 -0.159200   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "824   0.754275 -0.496412  0.580912  0.143711  ... -0.014561  0.067737   \n",
      "4777 -3.045621 -2.096166 -6.445610  2.422536  ...  0.667310  1.269205   \n",
      "340   0.520267 -0.259157  0.806329 -0.016192  ...  0.137318 -0.200431   \n",
      "4933  1.447043 -1.442584 -0.898702  0.123062  ...  0.313332  0.209086   \n",
      "742   0.261325 -0.617939  1.224288 -0.844641  ...  0.129803 -0.085056   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "955  -1.620135  0.743998  0.225636  0.720140  ... -0.423730  0.048145   \n",
      "2856  0.417636 -1.218769  1.165600 -0.413178  ...  0.048417  0.028063   \n",
      "4289  1.353603  3.312939 -1.029550  0.682809  ...  0.438181 -0.213804   \n",
      "4059 -0.979557  0.063401 -0.692738  0.126376  ... -0.089563 -0.123622   \n",
      "1563  1.427171 -0.856862  0.199476 -0.340134  ... -0.464608 -0.323045   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "824   0.056591  0.032287  0.035178  0.098192 -0.489207  0.090101  0.137386   \n",
      "4777  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274 -0.127944   \n",
      "340  -0.116354 -0.398652 -0.034692  0.444578 -0.489118  0.259927  0.065547   \n",
      "4933 -0.425938 -0.154440 -0.018820  0.632234  0.192922  0.468181  0.280486   \n",
      "742   0.177107 -0.142910  0.466897 -0.438533  0.803909 -0.907362 -0.614642   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "955   0.310025  0.109136  0.072415  0.178581 -0.283594 -0.559090 -0.286753   \n",
      "2856  0.465706 -0.358044 -0.108064  0.140817 -0.808643  0.238436 -0.018664   \n",
      "4289 -0.954272 -0.012297  0.925467  0.318062 -0.519700 -0.039983  0.033919   \n",
      "4059 -0.018446 -0.014677  0.477498  0.324373 -0.481738  0.035984  0.030464   \n",
      "1563 -0.418547 -0.487299 -0.387544 -0.215703  0.062328 -0.181186 -0.240170   \n",
      "\n",
      "      Class  \n",
      "824       0  \n",
      "4777      1  \n",
      "340       0  \n",
      "4933      1  \n",
      "742       0  \n",
      "...     ...  \n",
      "955       0  \n",
      "2856      0  \n",
      "4289      0  \n",
      "4059      0  \n",
      "1563      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4777      -0.046030    36.273296 -4.868108  1.264420 -5.167885  3.193648   \n",
      "4933      -0.229466    64.089782  1.261324  2.726800 -5.435019  5.342759   \n",
      "4720       4.789528    27.228844 -6.133987  2.941499 -5.593986  3.258845   \n",
      "4942       0.040897    64.802327  1.833191  0.745333 -1.133009  3.893556   \n",
      "4539       0.263347     4.400176 -4.727713  3.044469 -5.598354  5.928191   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "1480       0.047741    -0.401493  1.150559  0.217028  0.503593  1.066363   \n",
      "1380       2.446954    -0.438371 -2.248488 -0.195670  0.026949 -0.525780   \n",
      "2377       2.922998    -0.062123 -1.161086 -0.079347  1.010443  0.243638   \n",
      "2323       0.052361    -0.090221  1.163942 -0.409313 -0.353590  0.600512   \n",
      "1297       0.307495    -0.462957  1.293271 -0.686658  0.763193 -0.750240   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V19       V20  \\\n",
      "4777 -3.045621 -2.096166 -6.445610  2.422536  ...  1.385610  0.667310   \n",
      "4933  1.447043 -1.442584 -0.898702  0.123062  ... -1.466911  0.313332   \n",
      "4720 -5.315512 -0.637328 -4.476488  1.695994  ...  1.763611 -0.815086   \n",
      "4942  0.858164  0.910235 -0.498200  0.344703  ... -1.616927 -0.085579   \n",
      "4539 -2.190770 -1.529323 -4.487422  0.916392  ...  1.504925 -0.207759   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "1480 -0.461525 -0.745174  0.041322 -0.021768  ... -0.063033 -0.167924   \n",
      "1380 -0.238759 -0.033359  0.162196  0.928520  ...  0.516124 -0.082562   \n",
      "2377  0.848064  0.524867  1.104558  0.186342  ... -0.454843  0.360198   \n",
      "2323  1.614795  4.388608 -1.034458  1.149803  ...  0.903703 -0.045551   \n",
      "1297 -1.216752 -0.267433 -0.862741  0.037486  ... -0.480060 -0.360153   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4777  1.269205  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274   \n",
      "4933  0.209086 -0.425938 -0.154440 -0.018820  0.632234  0.192922  0.468181   \n",
      "4720  0.868340  0.793736  0.217347 -0.021985  0.145882  0.665088 -1.684186   \n",
      "4942  0.039289  0.181652  0.072981 -0.155299 -0.149891  0.012792  0.040854   \n",
      "4539  0.650988  0.254983  0.628843 -0.238128 -0.671332 -0.033590 -1.331777   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1480 -0.111975 -0.421642  0.057032  0.475331  0.380089 -0.580347  0.007138   \n",
      "1380 -0.032208 -0.745757 -0.243193 -0.821633 -0.030402  0.335838 -0.286998   \n",
      "2377  0.167278  0.199204  0.217637 -0.665093  0.185581 -0.504411  0.065101   \n",
      "2323 -0.493230 -1.151827  0.014007  0.995618  0.651431 -0.524167  0.075047   \n",
      "1297 -0.423452 -0.830487  0.123544  0.026530 -0.072321  0.855177 -0.036263   \n",
      "\n",
      "           V28  \n",
      "4777 -0.127944  \n",
      "4933  0.280486  \n",
      "4720  0.310195  \n",
      "4942  0.022903  \n",
      "4539  0.705698  \n",
      "...        ...  \n",
      "1480  0.018426  \n",
      "1380 -0.243768  \n",
      "2377  0.140659  \n",
      "2323  0.026361  \n",
      "1297  0.013303  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 1.0429e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 9.8504e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 1.2680e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 7.6276e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 6.1388e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 9.8345e-04 - accuracy: 1.0000 - val_loss: 5.9761e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 8.4643e-04 - accuracy: 1.0000 - val_loss: 5.7587e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 7.4012e-04 - accuracy: 1.0000 - val_loss: 5.5484e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 6.7578e-04 - accuracy: 1.0000 - val_loss: 5.3860e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 6.3122e-04 - accuracy: 1.0000 - val_loss: 5.2936e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 5.2936e-05 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time         V1         V2         V3        V4  \\\n",
      "85         3.165811    -0.885743  -4.575093  -4.429184   3.402585  0.903915   \n",
      "4758       4.102669    32.819010  -6.616293   3.563428  -7.058901  4.284346   \n",
      "1911       0.531828    -0.263637  -0.814123   0.727476   1.621366  0.727475   \n",
      "4885      -0.222450    56.018220   0.753356   2.284988  -5.164492  3.831112   \n",
      "2905      -0.085729     0.164856   1.220338  -0.019368   0.889428  1.110236   \n",
      "...             ...          ...        ...        ...        ...       ...   \n",
      "2295       0.169405    -0.102074   1.038611  -0.559941   1.651324  1.639150   \n",
      "1476      -0.239562    -0.403249   1.247541   1.251792  -1.602325  1.653127   \n",
      "4709       5.205339    25.333992  -8.257111  -4.814461  -5.365307  1.204230   \n",
      "4572       1.454312    10.870596 -20.532751  12.373989 -23.009003  6.144821   \n",
      "532        0.003251    -0.734716   1.091666   0.068682   1.230334  2.554474   \n",
      "\n",
      "             V5        V6         V7         V8  ...       V20       V21  \\\n",
      "85     3.002224 -0.491078  -2.705393   0.666451  ... -0.960963 -0.047365   \n",
      "4758  -5.096299 -1.768618  -4.937554   2.748460  ... -0.541634  1.215976   \n",
      "1911   0.563880 -0.278734   0.709642   0.020798  ...  0.263094  0.170308   \n",
      "4885  -0.073622 -1.316596  -1.855495   0.831079  ...  0.285792  0.382007   \n",
      "2905  -0.460948  0.368122  -0.543361   0.122383  ... -0.066684 -0.119578   \n",
      "...         ...       ...        ...        ...  ...       ...       ...   \n",
      "2295  -1.292460  0.985490  -1.207915   0.564181  ... -0.203912 -0.044650   \n",
      "1476   0.777110 -1.845478   0.729759  -0.347520  ... -0.080438 -0.218636   \n",
      "4709  -3.347420 -1.331601  -1.967893   1.295438  ... -1.233987  0.436390   \n",
      "4572 -15.587296 -4.384491 -15.939003  13.696416  ...  1.592754  1.754608   \n",
      "532   -0.454169  0.915781  -0.701016   0.396866  ... -0.216638 -0.098440   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "85    0.853360 -0.971600 -0.114862  0.408300 -0.304576  0.547785 -0.456297   \n",
      "4758  0.041178 -1.059098  0.275662  0.057425 -0.265838 -0.514637  0.388590   \n",
      "1911  0.328135 -0.286202  0.027621  0.570779 -0.195016  0.045185  0.071284   \n",
      "4885  0.033958  0.187697  0.358433 -0.488934 -0.258802  0.296145 -0.047174   \n",
      "2905 -0.107971 -0.131100 -0.560655  0.562442 -0.350990  0.074617  0.029776   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2295  0.242826 -0.145980  0.020555  0.518297 -0.219250  0.092627  0.025117   \n",
      "1476 -0.562316 -0.200743  0.036246  0.834931 -0.291626  0.035468  0.095443   \n",
      "4709 -0.077553 -3.091624 -0.390201 -0.288689 -0.340004  0.039819 -1.007900   \n",
      "4572 -1.466115 -0.856779  0.125777  1.402587 -0.223755  1.574249  0.469201   \n",
      "532  -0.178428  0.007277 -0.504025  0.255974  0.022213  0.048714  0.027057   \n",
      "\n",
      "      Class  \n",
      "85        0  \n",
      "4758      1  \n",
      "1911      0  \n",
      "4885      1  \n",
      "2905      0  \n",
      "...     ...  \n",
      "2295      0  \n",
      "1476      0  \n",
      "4709      1  \n",
      "4572      1  \n",
      "532       0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3        V4  \\\n",
      "4758       4.102669    32.819010  -6.616293  3.563428  -7.058901  4.284346   \n",
      "4885      -0.222450    56.018220   0.753356  2.284988  -5.164492  3.831112   \n",
      "4896      15.576831    58.257710  -0.967767  2.098019  -5.222929  6.514573   \n",
      "4676       1.454312    21.548677 -11.205461  7.914633 -13.987752  4.333341   \n",
      "4756       1.452772    32.446713  -5.140723  3.568751  -5.896245  4.164720   \n",
      "...             ...          ...        ...       ...        ...       ...   \n",
      "2520      -0.239562     0.008561  -0.278959  0.646643   1.220160 -0.892089   \n",
      "3045      -0.210643     0.235539  -0.468587  1.079763   1.693916 -0.110240   \n",
      "3888       0.045859     0.608276   1.152838 -0.159998   0.667810 -0.166714   \n",
      "3873      -0.181383     0.593349  -1.844120  1.493511   1.414584  1.255638   \n",
      "7          0.441478    -0.906816  -0.644269  1.417964   1.074380 -0.492199   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V19       V20  \\\n",
      "4758 -5.096299 -1.768618 -4.937554  2.748460  ...  2.143502 -0.541634   \n",
      "4885 -0.073622 -1.316596 -1.855495  0.831079  ...  2.362291  0.285792   \n",
      "4896 -4.187674  2.114178  0.948701 -2.448427  ...  0.952141  1.147204   \n",
      "4676 -8.484970 -3.506561 -8.935243  7.704449  ...  0.469554  0.860912   \n",
      "4756 -4.091193 -1.989960 -5.472436  2.422821  ...  1.737976  0.318995   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "2520  0.019214 -0.985625  0.629303 -0.178905  ...  1.168707  0.086175   \n",
      "3045 -0.019649 -0.664751  0.678538  0.007483  ...  0.219287  0.168495   \n",
      "3888 -0.655924 -0.346184 -0.356477  0.095971  ... -0.123230 -0.055579   \n",
      "3873  0.051745 -0.541168  0.805225 -0.686486  ...  1.542782 -0.103092   \n",
      "7     0.948934  0.428118  1.120631 -3.807864  ...  0.324505 -0.156742   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4758  1.215976  0.041178 -1.059098  0.275662  0.057425 -0.265838 -0.514637   \n",
      "4885  0.382007  0.033958  0.187697  0.358433 -0.488934 -0.258802  0.296145   \n",
      "4896 -0.843268  0.796739  1.314312 -0.352887 -1.770706  0.098133  0.956769   \n",
      "4676  0.942593 -0.987848 -0.279446 -0.027299  0.644344 -0.263078  1.084023   \n",
      "4756  1.131130  0.118022 -0.332704  0.139941  0.324758 -0.180769  0.177810   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2520  0.222747  0.847161 -0.186965  0.452004 -0.106871 -0.523168  0.206507   \n",
      "3045 -0.150869 -0.358060 -0.022338  0.545286 -0.215801  0.037574  0.268735   \n",
      "3888 -0.030664 -0.107994  0.124146  0.277474 -0.015418  0.902612 -0.060023   \n",
      "3873  0.052900  0.332851 -0.124460  0.400639  0.362232 -0.162329 -1.098938   \n",
      "7     1.943465 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921   \n",
      "\n",
      "           V28  \n",
      "4758  0.388590  \n",
      "4885 -0.047174  \n",
      "4896  0.162777  \n",
      "4676  0.211933  \n",
      "4756  0.661555  \n",
      "...        ...  \n",
      "2520  0.148543  \n",
      "3045  0.115127  \n",
      "3888  0.000707  \n",
      "3873 -0.062638  \n",
      "7    -1.085339  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 8.4880e-04 - accuracy: 1.0000 - val_loss: 7.6831e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 7.1908e-04 - accuracy: 1.0000 - val_loss: 6.9944e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 6.4662e-04 - accuracy: 1.0000 - val_loss: 6.6218e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 6.0138e-04 - accuracy: 1.0000 - val_loss: 6.4656e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 5.6468e-04 - accuracy: 1.0000 - val_loss: 6.1772e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 5.2465e-04 - accuracy: 1.0000 - val_loss: 6.2310e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 5.0142e-04 - accuracy: 1.0000 - val_loss: 6.0683e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4.6910e-04 - accuracy: 1.0000 - val_loss: 5.7185e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4.5366e-04 - accuracy: 1.0000 - val_loss: 5.7050e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4.2821e-04 - accuracy: 1.0000 - val_loss: 5.3656e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.3656e-05 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "3133       3.319644     0.279881 -0.733264  0.220740  2.154380  0.043069   \n",
      "3846      -0.057153     0.574031 -1.660254  1.896936 -0.035586  0.781848   \n",
      "2776      -0.241444     0.108221  0.154568  0.941367  1.417823  1.053450   \n",
      "552        0.727242    -0.728570  1.146594 -0.353208  0.834120  0.708139   \n",
      "76         0.769849    -0.888377 -0.549626  0.418949  1.729833  0.203065   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "1963       0.085387    -0.246954 -0.372594  0.291697  1.437615 -1.442014   \n",
      "2022       1.255989    -0.220173 -0.907342  0.744462  2.301932  1.517583   \n",
      "1454       0.961157    -0.414225 -0.403358  0.307439  1.886847  0.901240   \n",
      "1898      -0.171116    -0.265833 -1.629826  1.326529  0.906349 -0.800461   \n",
      "3619       1.779603     0.449347 -0.249964 -0.371521  1.410132 -1.126919   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "3133 -1.042791  0.842349  0.960767 -0.059609  ...  0.049009 -0.022857   \n",
      "3846 -0.598215 -0.402011 -0.368105  1.144671  ... -0.341362  0.368000   \n",
      "2776  0.039278 -0.717523  0.639908 -0.350254  ...  0.251350 -0.234494   \n",
      "552  -0.749014  0.189676 -0.593212  0.126902  ...  0.011619 -0.120062   \n",
      "76   -0.187012  0.253878  0.500894  0.251256  ...  0.016970  0.115062   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "1963  0.028033 -0.117853  0.434629  0.173717  ... -0.043671 -0.117484   \n",
      "2022 -0.726188  1.089493  0.496078  0.165447  ...  0.188409 -0.480747   \n",
      "1454 -0.332366 -0.261843  0.706182 -0.103885  ...  0.157012 -0.076218   \n",
      "1898 -0.283720 -0.383247  0.201000  0.585984  ...  0.080721 -0.197779   \n",
      "3619 -1.219167  0.878293 -0.493258  0.146646  ...  0.147753  0.499965   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "3133  0.219450 -0.091515  0.028696 -0.283616  0.236626 -0.098537 -0.166565   \n",
      "3846  0.539893 -0.094375  0.024265 -0.206283 -0.366751 -0.609295 -0.046866   \n",
      "2776 -0.451775  0.022609  0.362480 -0.656059  0.242236 -0.078416 -0.137300   \n",
      "552  -0.268422 -0.112320 -0.410470  0.341273  0.364006  0.005175  0.026810   \n",
      "76    0.418529 -0.065133  0.264981  0.003958  0.395969  0.027182  0.043506   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1963 -0.262198 -0.019567 -0.334041 -0.413197  0.734334  0.003457 -0.176634   \n",
      "2022 -0.554517 -0.074535  0.059955 -0.064549 -0.400035  0.300230 -0.002793   \n",
      "1454 -0.031594  0.154256  0.597104 -0.554755  0.311924 -0.057622 -0.077744   \n",
      "1898 -0.385304 -0.066693  0.022864  0.141284  0.385286  0.250911  0.192462   \n",
      "3619  1.472650 -0.040344 -0.785061 -0.361566 -0.005049 -0.075794  0.026576   \n",
      "\n",
      "      Class  \n",
      "3133      0  \n",
      "3846      0  \n",
      "2776      0  \n",
      "552       0  \n",
      "76        0  \n",
      "...     ...  \n",
      "1963      0  \n",
      "2022      0  \n",
      "1454      0  \n",
      "1898      0  \n",
      "3619      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3         V4  \\\n",
      "4553      -0.239562     7.167819 -14.474437  6.503185 -17.712632  11.270352   \n",
      "4886      -0.239562    56.050269  -1.396204  2.618584  -6.036770   3.552454   \n",
      "4812       2.969199    40.297443 -10.632375  7.251936 -17.681072   8.204144   \n",
      "4844       2.273614    43.694216   0.913116  1.145381  -4.602878   2.091803   \n",
      "4635      -0.243669    17.225552 -13.680760  6.990389 -13.770001   8.694897   \n",
      "...             ...          ...        ...       ...        ...        ...   \n",
      "3150       0.169405     0.287784   0.957784 -0.593668   1.849287   1.716378   \n",
      "3456       1.365503     0.381736  -0.455382  0.465230   1.478102   1.218583   \n",
      "602        1.822040    -0.709253  -0.469790 -0.009723   1.759079  -0.086506   \n",
      "2554      -0.239562     0.014268  -0.643331  0.876486   1.114308  -2.067565   \n",
      "3928       0.707050     0.641203  -0.551627  0.880860   1.960172   1.290978   \n",
      "\n",
      "             V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4553  -4.150142 -3.372098 -16.535807 -1.443947  ...  0.579200  1.101250   \n",
      "4886   1.030091 -2.950358  -1.528506  0.189319  ...  0.170144  0.031795   \n",
      "4812 -10.166591 -4.510344 -12.981606  6.783589  ...  3.813041 -0.810146   \n",
      "4844  -0.473224 -2.085436  -1.671244  0.094397  ...  0.252274  0.508985   \n",
      "4635 -11.426968 -2.919845 -14.594562  8.622905  ...  3.517611 -0.877392   \n",
      "...         ...       ...        ...       ...  ...       ...       ...   \n",
      "3150  -1.369590  1.143323  -1.273025  0.666695  ...  0.034453 -0.253238   \n",
      "3456   0.393910  0.084723   1.075616 -0.220123  ...  0.415437  0.295604   \n",
      "602   -0.508940  0.757607   0.256744  0.208422  ...  1.084289  0.367977   \n",
      "2554   0.181702 -1.228307   1.117487 -0.333802  ... -0.615700  0.140372   \n",
      "3928   0.141388  0.048400   1.178933 -0.401107  ...  0.643690  0.237686   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4553 -2.475962  0.342391 -3.564508 -0.818140  0.153408  0.755079  2.706566   \n",
      "4886  0.143177 -0.390176  0.356029 -0.762352  0.096510 -0.487861  0.062655   \n",
      "4812  2.715357  0.695603 -1.138122  0.459442  0.386337  0.522438 -1.416604   \n",
      "4844  0.401416 -0.084933 -0.197684 -0.283271  0.387597 -0.553527  0.516274   \n",
      "4635  2.989553  0.497599 -0.509290  0.732503  0.280528  0.280037 -1.406687   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3150  0.018127  0.456629 -0.051369  0.228856  0.374209 -0.221942  0.109926   \n",
      "3456  0.049360  0.392740  0.067609  0.106963 -0.240801 -0.352548  0.004406   \n",
      "602  -0.030373  0.050121  0.248683 -0.373362 -0.725709  0.992874  0.106658   \n",
      "2554  0.084476  0.735502 -0.218425  0.411130  0.019755 -0.826653  0.362879   \n",
      "3928 -0.138282  0.276980 -0.252771  0.411485  0.072293 -0.260608 -0.035620   \n",
      "\n",
      "           V28  \n",
      "4553 -0.992916  \n",
      "4886 -0.240732  \n",
      "4812 -0.488307  \n",
      "4844  0.270828  \n",
      "4635 -0.663643  \n",
      "...        ...  \n",
      "3150  0.026982  \n",
      "3456 -0.052381  \n",
      "602   0.160211  \n",
      "2554  0.047016  \n",
      "3928 -0.291230  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 4.7602e-04 - accuracy: 1.0000 - val_loss: 3.2762e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 4.2672e-04 - accuracy: 1.0000 - val_loss: 3.0064e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3.8867e-04 - accuracy: 1.0000 - val_loss: 2.8485e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3.6171e-04 - accuracy: 1.0000 - val_loss: 2.5954e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3.4757e-04 - accuracy: 1.0000 - val_loss: 2.4771e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 3.3385e-04 - accuracy: 1.0000 - val_loss: 2.3992e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3.2212e-04 - accuracy: 1.0000 - val_loss: 2.2538e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3.0671e-04 - accuracy: 1.0000 - val_loss: 2.1435e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3.0708e-04 - accuracy: 1.0000 - val_loss: 2.0279e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 2.8499e-04 - accuracy: 1.0000 - val_loss: 1.9934e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.9934e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "for train_val_index, test_val_index in sss.split(data_Xtrain, data_ytrain):\n",
    "    X_train, X_val = data_Xtrain.iloc[train_val_index], data_Xtrain.iloc[test_val_index]\n",
    "    y_train, y_val = data_ytrain.iloc[train_val_index], data_ytrain.iloc[test_val_index]\n",
    "\n",
    "    train_val_data = X_train\n",
    "    train_val_data = train_val_data.assign(Class=y_train)\n",
    "    print(train_val_data)\n",
    "\n",
    "    fraud_df = train_val_data.loc[train_val_data['Class'] == 1]\n",
    "    non_fraud_df = train_val_data.loc[train_val_data['Class'] == 0].sample(len(fraud_df))\n",
    "    print(f'Relation fraud non-fraud in the CV set: {len(fraud_df)} x {len(non_fraud_df)}')\n",
    "\n",
    "    print(f'Test training relationship {len(notfraud_df)+len(fraud_df)} x {len(y_val)}')\n",
    "    print(f'Test training relationship {(len(notfraud_df)+len(fraud_df))/(len(non_fraud_df)+len(fraud_df)+len(y_val))} x {len(y_val)/(len(non_fraud_df)+len(fraud_df)+len(y_val))}')\n",
    "    \n",
    "    X_train_resampled = pd.concat([fraud_df, non_fraud_df]).drop('Class', axis=1)\n",
    "    y_train_resampled = pd.concat([fraud_df, non_fraud_df])['Class']\n",
    "    \n",
    "    print(X_train_resampled)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Fit the model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    history = model.fit(x=X_train_scaled, y=y_train_resampled, epochs=10, validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=[early_stop])\n",
    "\n",
    "    history_list.append(history.history)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_scaled, y_val)\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    accuracy_scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy Neural Network: 1.0\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1352\n",
      "           1       0.99      0.98      0.99       148\n",
      "\n",
      "    accuracy                           1.00      1500\n",
      "   macro avg       1.00      0.99      0.99      1500\n",
      "weighted avg       1.00      1.00      1.00      1500\n",
      "\n",
      "Recall:  0.9797297297297297\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the average accuracy across all folds\n",
    "print(f\"Average Accuracy Neural Network: {np.mean(accuracy_scores)}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_scaled = scaler.transform(data_Xtest)\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(data_ytest, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_ytest, y_pred))\n",
    "print(\"Recall: \", recall_score(data_ytest, y_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0, 1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
