{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaraies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load The dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>169142.0</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>390.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>169347.0</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>169351.0</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>77.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>169966.0</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>245.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>170348.0</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>42.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0          0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1          0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2          1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3          1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4          2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4995  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "4996  169347.0  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
       "4997  169351.0 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346   \n",
       "4998  169966.0 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
       "4999  170348.0  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1    -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2     0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3     0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4     0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4995 -0.882850  0.697211 -2.064945  ...  0.778584 -0.319189  0.639419   \n",
       "4996 -1.413170  0.248525 -1.127396  ...  0.370612  0.028234 -0.145640   \n",
       "4997 -2.234739  1.210158 -0.652250  ...  0.751826  0.834108  0.190944   \n",
       "4998 -2.208002  1.058733 -1.632333  ...  0.583276 -0.269209 -0.456108   \n",
       "4999  0.223050 -0.068384  0.577829  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1    -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2    -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3    -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4     0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "4995 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00      1  \n",
       "4996 -0.081049  0.521875  0.739467  0.389152  0.186637    0.76      1  \n",
       "4997  0.032070 -0.739695  0.471111  0.385107  0.194361   77.89      1  \n",
       "4998 -0.183659 -0.328168  0.606116  0.884876 -0.253700  245.00      1  \n",
       "4999 -0.450261  0.313267 -0.289617  0.002988 -0.015309   42.53      1  \n",
       "\n",
       "[5000 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Datasets\\FinalCreditCard.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is scaled Already, except the Amount and Time colomns are left to scale. so produdre we will follow in Preprocessing phase as follow:\n",
    "* using RobustScaler to scale Time and Amount columns because it is not effected by small number of huge marginal outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the fraud and not fruad data separtely \n",
    "fraud = data[data['Class'] == 1]\n",
    "notfraud = data[data['Class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "#Scale Time and Amount Outliers\n",
    "data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reomeve the coloumns before, becase we already done scaling\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount = data['scaled_amount']\n",
    "scaled_time = data['scaled_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "\n",
    "# Put the new columns in suitable order\n",
    "data.insert(0, 'scaled_amount', scaled_amount)\n",
    "data.insert(1, 'scaled_time', scaled_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.303559</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.210643</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.222793</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856605</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940965</td>\n",
       "      <td>-0.909011</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       2.303559    -0.909889 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.210643    -0.909889  1.191857  0.266151  0.166480  0.448154   \n",
       "2       6.222793    -0.909450 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.856605    -0.909450 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.940965    -0.909011 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the data after is completely scaled \n",
    "fraud = data[data['Class'] == 1]\n",
    "notfraud = data[data['Class'] == 0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase \n",
    "As well as our dataset is clearly unbalanced, we will utilize StratifiedShuffleSplit() function. this function used to split data train/test sets by their indices. StratifiedShuffleSplit() is Cross-Validation object combine between two things, which are: ShuffleSplit and StratifiedKFold, and it produces Stratified Randomize Folds. Those folds are made by maintain the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Frauds 90.16 % of the dataset\n",
      "Frauds 9.84 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# see the percentages of fraud and non fraud  \n",
    "\n",
    "print('Not Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.303559</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.210643</td>\n",
       "      <td>-0.909889</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.222793</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856605</td>\n",
       "      <td>-0.909450</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940965</td>\n",
       "      <td>-0.909011</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>6.416838</td>\n",
       "      <td>73.348480</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391167</td>\n",
       "      <td>1.252967</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.243669</td>\n",
       "      <td>73.438481</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737657</td>\n",
       "      <td>0.226138</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1.076146</td>\n",
       "      <td>73.440237</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266272</td>\n",
       "      <td>0.247968</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>3.935661</td>\n",
       "      <td>73.710240</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035480</td>\n",
       "      <td>0.306271</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.471081</td>\n",
       "      <td>73.877950</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593509</td>\n",
       "      <td>-0.017652</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0          2.303559    -0.909889 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1         -0.210643    -0.909889  1.191857  0.266151  0.166480  0.448154   \n",
       "2          6.222793    -0.909450 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3          1.856605    -0.909450 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4          0.940965    -0.909011 -1.158233  0.877737  1.548718  0.403034   \n",
       "...             ...          ...       ...       ...       ...       ...   \n",
       "4995       6.416838    73.348480 -1.927883  1.125653 -4.518331  1.749293   \n",
       "4996      -0.243669    73.438481  1.378559  1.289381 -5.004247  1.411850   \n",
       "4997       1.076146    73.440237 -0.676143  1.126366 -2.213700  0.468308   \n",
       "4998       3.935661    73.710240 -3.113832  0.585864 -5.399730  1.817092   \n",
       "4999       0.471081    73.877950  1.991976  0.158476 -2.583441  0.408670   \n",
       "\n",
       "            V5        V6        V7        V8  ...       V19       V20  \\\n",
       "0    -0.338321  0.462388  0.239599  0.098698  ...  0.403993  0.251412   \n",
       "1     0.060018 -0.082361 -0.078803  0.085102  ... -0.145783 -0.069083   \n",
       "2    -0.503198  1.800499  0.791461  0.247676  ... -2.261857  0.524980   \n",
       "3    -0.010309  1.247203  0.237609  0.377436  ... -1.232622 -0.208038   \n",
       "4    -0.407193  0.095921  0.592941 -0.270533  ...  0.803487  0.408542   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "4995 -1.566487 -2.010494 -0.882850  0.697211  ...  0.391167  1.252967   \n",
       "4996  0.442581 -1.326536 -1.413170  0.248525  ...  0.737657  0.226138   \n",
       "4997 -1.120541 -0.003346 -2.234739  1.210158  ...  0.266272  0.247968   \n",
       "4998 -0.840618 -2.943548 -2.208002  1.058733  ... -0.035480  0.306271   \n",
       "4999  1.151147 -0.096695  0.223050 -0.068384  ...  0.593509 -0.017652   \n",
       "\n",
       "           V21       V22       V23       V24       V25       V26       V27  \\\n",
       "0    -0.018307  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558   \n",
       "1    -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983   \n",
       "2     0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353   \n",
       "3    -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723   \n",
       "4    -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4995  0.778584 -0.319189  0.639419 -0.294885  0.537503  0.788395  0.292680   \n",
       "4996  0.370612  0.028234 -0.145640 -0.081049  0.521875  0.739467  0.389152   \n",
       "4997  0.751826  0.834108  0.190944  0.032070 -0.739695  0.471111  0.385107   \n",
       "4998  0.583276 -0.269209 -0.456108 -0.183659 -0.328168  0.606116  0.884876   \n",
       "4999 -0.164350 -0.295135 -0.072173 -0.450261  0.313267 -0.289617  0.002988   \n",
       "\n",
       "           V28  \n",
       "0    -0.021053  \n",
       "1     0.014724  \n",
       "2    -0.059752  \n",
       "3     0.061458  \n",
       "4     0.215153  \n",
       "...        ...  \n",
       "4995  0.147968  \n",
       "4996  0.186637  \n",
       "4997  0.194361  \n",
       "4998 -0.253700  \n",
       "4999 -0.015309  \n",
       "\n",
       "[5000 rows x 30 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting 70% of the datasets \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [4327 4175  282 ... 1837 1914 3282] Test: [1930 2723 1968 ... 2956 3674 3090]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    data_Xtrain, data_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    data_ytrain, data_ytest = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Label Distributions: [Fraud Not-Fraud]\n",
      "Train: \n",
      "[0.90171429 0.09828571]\n",
      "[3156  344]\n",
      "Test: \n",
      "[0.90133333 0.09866667]\n",
      "[1352  148]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>0.057837</td>\n",
       "      <td>0.740863</td>\n",
       "      <td>1.347239</td>\n",
       "      <td>-0.468724</td>\n",
       "      <td>0.659279</td>\n",
       "      <td>-0.494070</td>\n",
       "      <td>-0.960005</td>\n",
       "      <td>-0.556445</td>\n",
       "      <td>-0.771329</td>\n",
       "      <td>-0.084322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>-0.051700</td>\n",
       "      <td>-0.223821</td>\n",
       "      <td>-0.554318</td>\n",
       "      <td>0.179944</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>0.096299</td>\n",
       "      <td>-0.487627</td>\n",
       "      <td>-0.008361</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.102669</td>\n",
       "      <td>0.736033</td>\n",
       "      <td>1.421832</td>\n",
       "      <td>-0.492554</td>\n",
       "      <td>-0.023678</td>\n",
       "      <td>-1.024934</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.785589</td>\n",
       "      <td>-0.831478</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982620</td>\n",
       "      <td>0.092759</td>\n",
       "      <td>-0.271269</td>\n",
       "      <td>-0.669532</td>\n",
       "      <td>-0.062000</td>\n",
       "      <td>-1.794172</td>\n",
       "      <td>0.333973</td>\n",
       "      <td>-0.401932</td>\n",
       "      <td>-0.007592</td>\n",
       "      <td>-0.012509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1.095140</td>\n",
       "      <td>-0.822083</td>\n",
       "      <td>-1.269461</td>\n",
       "      <td>-1.027153</td>\n",
       "      <td>2.261373</td>\n",
       "      <td>-0.504356</td>\n",
       "      <td>0.693421</td>\n",
       "      <td>1.237761</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.091919</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.463993</td>\n",
       "      <td>-0.527727</td>\n",
       "      <td>-0.131777</td>\n",
       "      <td>0.697266</td>\n",
       "      <td>0.288501</td>\n",
       "      <td>-0.644487</td>\n",
       "      <td>0.027409</td>\n",
       "      <td>-0.329714</td>\n",
       "      <td>-0.291795</td>\n",
       "      <td>-0.381556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.171116</td>\n",
       "      <td>-0.231588</td>\n",
       "      <td>-0.693979</td>\n",
       "      <td>0.863780</td>\n",
       "      <td>1.782080</td>\n",
       "      <td>-0.621203</td>\n",
       "      <td>-0.034457</td>\n",
       "      <td>-0.556248</td>\n",
       "      <td>0.704357</td>\n",
       "      <td>0.061961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121181</td>\n",
       "      <td>0.081262</td>\n",
       "      <td>-0.124982</td>\n",
       "      <td>-0.122718</td>\n",
       "      <td>-0.158336</td>\n",
       "      <td>0.590370</td>\n",
       "      <td>-0.014215</td>\n",
       "      <td>0.344616</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.157360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>0.411533</td>\n",
       "      <td>0.057732</td>\n",
       "      <td>-0.528280</td>\n",
       "      <td>1.297138</td>\n",
       "      <td>1.497918</td>\n",
       "      <td>1.128073</td>\n",
       "      <td>1.015072</td>\n",
       "      <td>0.182683</td>\n",
       "      <td>1.317492</td>\n",
       "      <td>-0.331670</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.154711</td>\n",
       "      <td>-0.009357</td>\n",
       "      <td>-0.056918</td>\n",
       "      <td>-0.080751</td>\n",
       "      <td>-0.286461</td>\n",
       "      <td>-0.652466</td>\n",
       "      <td>-0.042145</td>\n",
       "      <td>-0.245139</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>-0.102000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "4327       0.057837     0.740863  1.347239 -0.468724  0.659279 -0.494070   \n",
       "4175       0.102669     0.736033  1.421832 -0.492554 -0.023678 -1.024934   \n",
       "282        1.095140    -0.822083 -1.269461 -1.027153  2.261373 -0.504356   \n",
       "1999      -0.171116    -0.231588 -0.693979  0.863780  1.782080 -0.621203   \n",
       "2662       0.411533     0.057732 -0.528280  1.297138  1.497918  1.128073   \n",
       "\n",
       "            V5        V6        V7        V8  ...       V19       V20  \\\n",
       "4327 -0.960005 -0.556445 -0.771329 -0.084322  ...  0.049968 -0.051700   \n",
       "4175  0.038346  0.785589 -0.831478  0.108989  ...  0.982620  0.092759   \n",
       "282   0.693421  1.237761  0.056065  0.091919  ... -3.463993 -0.527727   \n",
       "1999 -0.034457 -0.556248  0.704357  0.061961  ...  0.121181  0.081262   \n",
       "2662  1.015072  0.182683  1.317492 -0.331670  ... -1.154711 -0.009357   \n",
       "\n",
       "           V21       V22       V23       V24       V25       V26       V27  \\\n",
       "4327 -0.223821 -0.554318  0.179944 -0.040405  0.096299 -0.487627 -0.008361   \n",
       "4175 -0.271269 -0.669532 -0.062000 -1.794172  0.333973 -0.401932 -0.007592   \n",
       "282  -0.131777  0.697266  0.288501 -0.644487  0.027409 -0.329714 -0.291795   \n",
       "1999 -0.124982 -0.122718 -0.158336  0.590370 -0.014215  0.344616  0.284906   \n",
       "2662 -0.056918 -0.080751 -0.286461 -0.652466 -0.042145 -0.245139 -0.188071   \n",
       "\n",
       "           V28  \n",
       "4327  0.012329  \n",
       "4175 -0.012509  \n",
       "282  -0.381556  \n",
       "1999  0.157360  \n",
       "2662 -0.102000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check the distribution of data\n",
    "train_unique_label, train_counts_label = np.unique(data_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(data_ytest, return_counts=True)\n",
    "print('-' * 50)\n",
    "\n",
    "print('Label Distributions: [Fraud Not-Fraud]')\n",
    "print('Train: ')\n",
    "print(train_counts_label/ len(data_ytrain))\n",
    "print(train_counts_label)\n",
    "print('Test: ')\n",
    "print(test_counts_label/ len(data_ytest))\n",
    "print(test_counts_label)\n",
    "\n",
    "data_Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3500 entries, 4327 to 3282\n",
      "Data columns (total 30 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   scaled_amount  3500 non-null   float64\n",
      " 1   scaled_time    3500 non-null   float64\n",
      " 2   V1             3500 non-null   float64\n",
      " 3   V2             3500 non-null   float64\n",
      " 4   V3             3500 non-null   float64\n",
      " 5   V4             3500 non-null   float64\n",
      " 6   V5             3500 non-null   float64\n",
      " 7   V6             3500 non-null   float64\n",
      " 8   V7             3500 non-null   float64\n",
      " 9   V8             3500 non-null   float64\n",
      " 10  V9             3500 non-null   float64\n",
      " 11  V10            3500 non-null   float64\n",
      " 12  V11            3500 non-null   float64\n",
      " 13  V12            3500 non-null   float64\n",
      " 14  V13            3500 non-null   float64\n",
      " 15  V14            3500 non-null   float64\n",
      " 16  V15            3500 non-null   float64\n",
      " 17  V16            3500 non-null   float64\n",
      " 18  V17            3500 non-null   float64\n",
      " 19  V18            3500 non-null   float64\n",
      " 20  V19            3500 non-null   float64\n",
      " 21  V20            3500 non-null   float64\n",
      " 22  V21            3500 non-null   float64\n",
      " 23  V22            3500 non-null   float64\n",
      " 24  V23            3500 non-null   float64\n",
      " 25  V24            3500 non-null   float64\n",
      " 26  V25            3500 non-null   float64\n",
      " 27  V26            3500 non-null   float64\n",
      " 28  V27            3500 non-null   float64\n",
      " 29  V28            3500 non-null   float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 847.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_Xtrain.info())\n",
    "#print(data_ytrain.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#to prints and plots the confusion matrix \n",
    "#Normilization is done by setting normalize=True\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model phase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the data loaded into original_Xtrain, original_ytrain, original_Xtest, and original_ytest\n",
    "\n",
    "# Initialize StratifiedShuffleSplit for cross-validation\n",
    "sss = StratifiedShuffleSplit(n_splits=7, test_size=0.0015, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=30))  # Update input_dim to 29\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = RobustScaler()\n",
    "history_list = []\n",
    "\n",
    "# Initialize an array to store accuracy scores for each fold\n",
    "accuracy_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [4778, 4882, 4730, 4674, 4638, 4593, 4989, 4602, 4579, 4978, 4532, 4773, 4818, 4831, 4572, 4865, 4595, 4782, 4873, 4817, 4994, 4557, 4954, 4528, 4993, 4560, 4533, 4512, 4768, 4806, 4608, 4735, 4641, 4620, 4683, 4615, 4543, 4686, 4957, 4791, 4981, 4738, 4905, 4871, 4943, 4976, 4767, 4892, 4656, 4740, 4896, 4797, 4654, 4960, 4836, 4977, 4710, 4794, 4834, 4644, 4605, 4956, 4633, 4558, 4519, 4975, 4544, 4531, 4852, 4990, 4765, 4965, 4811, 4915, 4545, 4585, 4945, 4712, 4691, 4665, 4800, 4671, 4606, 4627, 4660, 4723, 4621, 4726, 4813, 4707, 4944, 4573, 4525, 4521, 4574, 4759, 4844, 4898, 4648, 4810, ...]\n",
      "\n",
      "[686 rows x 0 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\iraha\\OneDrive - IMAM ABDULRAHMAN BIN FAISAL UNIVERSITY\\Desktop\\AI Levels\\Senior Level 1\\Projects\\DL Project\\CreditCardFraudDetection\\SubGroup2.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(X_train_resampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Scale the data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m X_train_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X_train_resampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m X_val_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iraha/OneDrive%20-%20IMAM%20ABDULRAHMAN%20BIN%20FAISAL%20UNIVERSITY/Desktop/AI%20Levels/Senior%20Level%201/Projects/DL%20Project/CreditCardFraudDetection/SubGroup2.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1533\u001b[0m, in \u001b[0;36mRobustScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the median and quantiles to be used for scaling.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \n\u001b[0;32m   1517\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39m# at fit, convert sparse matrices to csc for optimized computation of\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[39m# the quantiles\u001b[39;00m\n\u001b[1;32m-> 1533\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1534\u001b[0m     X,\n\u001b[0;32m   1535\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1536\u001b[0m     dtype\u001b[39m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1537\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1538\u001b[0m )\n\u001b[0;32m   1540\u001b[0m q_min, q_max \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantile_range\n\u001b[0;32m   1541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m q_min \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m q_max \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\iraha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:797\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    793\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    794\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    795\u001b[0m )\n\u001b[0;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 797\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mresult_type(\u001b[39m*\u001b[39mdtypes_orig)\n\u001b[0;32m    798\u001b[0m \u001b[39melif\u001b[39;00m pandas_requires_conversion \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(d \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    799\u001b[0m     \u001b[39m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    800\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for train_val_index, test_val_index in sss.split(data_Xtrain, data_ytrain):\n",
    "    X_train, X_val = data_Xtrain.iloc[train_val_index], data_Xtrain.iloc[test_val_index]\n",
    "    y_train, y_val = data_ytrain.iloc[train_val_index], data_ytrain.iloc[test_val_index]\n",
    "\n",
    "    #train_val_data =  pd.DataFrame(X_train)\n",
    "   \n",
    "    train_val_data = train_val_data.assign(Class=y_train)\n",
    "    #train_val_data=pd.DataFrame(train_val_data)\n",
    "\n",
    "    fraud_df = train_val_data.loc[train_val_data['Class'] == 1]\n",
    "    notfraud_df = train_val_data.loc[train_val_data['Class'] == 0].sample(len(fraud_df))\n",
    "\n",
    "    print(f'Relation fraud non-fraud in the CV set: {len(fraud_df)} x {len(non_fraud_df)}')\n",
    "\n",
    "    print(f'Test training relationship {len(notfraud_df)+len(fraud_df)} x {len(y_val)}')\n",
    "    print(f'Test training relationship {(len(notfraud_df)+len(fraud_df))/(len(non_fraud_df)+len(fraud_df)+len(y_val))} x {len(y_val)/(len(non_fraud_df)+len(fraud_df)+len(y_val))}')\n",
    "    \n",
    "        \n",
    "        \n",
    "    X_train_resampled = pd.concat([fraud_df, notfraud_df]).drop('Class', axis=1)\n",
    "    y_train_resampled = pd.concat([fraud_df, notfraud_df])['Class']\n",
    "    \n",
    "    print(X_train_resampled)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Fit the model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    history = model.fit(x=X_train_scaled, y=y_train_resampled, epochs=1000, validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=[early_stop])\n",
    "\n",
    "    history_list.append(history.history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_scaled, y_val)\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "print(f\"Average Accuracy Neural Network: {np.mean(accuracy_scores)}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_scaled = scaler.transform(data_Xtest)\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(data_ytest, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_ytest, y_pred))\n",
    "print(\"Recall: \", recall_score(data_ytest, y_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0, 1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4778       5.806468    36.315004 -1.430864 -0.802529  1.123320  0.389760   \n",
      "1845      -0.222793    -0.281638  1.192420  0.076970  0.583453  0.665919   \n",
      "2236      -0.234600    -0.133246  1.241908  0.205299  0.398826  0.424773   \n",
      "2865      -0.140828     0.156514 -0.436030  1.635475  1.647855  2.468624   \n",
      "1564       4.482033    -0.371200 -2.749525 -3.300879  1.933082  0.312397   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "3024       0.331622     0.221491 -1.335640  0.093758  1.716347  0.321310   \n",
      "4237       0.020363     0.738229  1.451138 -0.601564 -0.886202 -1.637269   \n",
      "1419       6.026694    -0.428274  0.483432 -1.533762  0.618801 -0.491250   \n",
      "1656      -0.008556    -0.345736 -3.835659 -1.955510 -0.404067  0.438873   \n",
      "648       -0.085558    -0.694326  1.130024 -0.060944  0.662074  1.033990   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "4778 -0.281214 -0.055123  1.326232  0.195700  ...  0.634184  0.325575   \n",
      "1845 -0.617114 -0.753429 -0.133273  0.016383  ... -0.252928 -0.229348   \n",
      "2236 -0.243974 -0.473666 -0.036665 -0.047231  ... -0.060969 -0.225866   \n",
      "2865  0.625894  0.347932  0.825010 -0.188491  ...  0.558709 -0.321424   \n",
      "1564  1.442330 -1.421397 -0.535266  0.097426  ...  0.784980 -0.211812   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "3024  0.597336 -1.024550  0.340861 -0.280405  ... -0.197070 -0.308835   \n",
      "4237  1.483689  3.265473 -1.204579  0.717045  ...  0.169572 -0.293726   \n",
      "1419 -1.570267 -0.450331 -0.171666 -0.039912  ...  0.676298  0.432930   \n",
      "1656  1.787822 -1.066112 -0.275514  0.221039  ... -1.180631 -0.661170   \n",
      "648  -0.441560  0.310187 -0.559948  0.349136  ... -0.258810  0.195852   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "4778  0.014002  0.844946  0.114963  0.156365 -0.619437 -0.120351  0.035594   \n",
      "1845 -0.720419  0.228258  0.344679  0.038809  0.110178 -0.024190  0.015007   \n",
      "2236 -0.678312  0.081198 -0.002696  0.237988  0.097226 -0.032047  0.005763   \n",
      "2865 -0.544905 -0.144442 -0.474564 -0.241057 -0.015167  0.330356  0.035417   \n",
      "1564 -1.222846  0.948554  0.196215  0.647151 -0.652061 -0.347416 -0.176205   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3024 -0.355747 -0.393202  0.419444 -0.127249  0.201127  0.049004 -0.085552   \n",
      "4237 -0.884656  0.103960  0.918699  0.366493 -0.505633 -0.015061  0.010953   \n",
      "1419  0.696581 -0.451252  0.613066  0.459382 -0.602441  0.027400  0.090133   \n",
      "1656  0.285619  1.336996 -0.688062  0.784588  1.281218  0.224189 -0.883502   \n",
      "648   0.511310 -0.100457 -0.344298  0.420079 -0.213097  0.040658  0.010599   \n",
      "\n",
      "      Class  \n",
      "4778      1  \n",
      "1845      0  \n",
      "2236      0  \n",
      "2865      0  \n",
      "1564      0  \n",
      "...     ...  \n",
      "3024      0  \n",
      "4237      0  \n",
      "1419      0  \n",
      "1656      0  \n",
      "648       0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3        V4  \\\n",
      "4778       5.806468    36.315004  -1.430864 -0.802529   1.123320  0.389760   \n",
      "4882      -0.173340    55.822412   1.177824  2.487103  -5.330608  5.324547   \n",
      "4730       8.123032    28.570958  -1.824295  0.403327  -1.994122  2.756558   \n",
      "4674       1.454312    21.529799  -9.848776  7.365546 -12.898538  4.273323   \n",
      "4638       4.414956    17.264625 -14.970346  8.401421 -16.867238  8.252334   \n",
      "...             ...          ...        ...       ...        ...       ...   \n",
      "1450      -0.239562    -0.415103  -1.380849 -0.145625   0.126665 -2.162818   \n",
      "3566       0.937714     0.428713  -1.307922  1.117889   1.206546 -0.769958   \n",
      "171        1.632444    -0.861596  -1.276830 -0.903557   1.477105 -2.582626   \n",
      "2050       0.164442    -0.213149   1.153677  0.216433   0.553173  1.532029   \n",
      "2046      -0.171116    -0.214905  -1.361981  1.186318  -0.513394 -1.797613   \n",
      "\n",
      "             V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4778  -0.281214 -0.055123   1.326232  0.195700  ... -1.140376  0.634184   \n",
      "4882   1.150243 -1.281843  -1.171994  0.413778  ... -1.359844  0.156057   \n",
      "4730  -3.139064  0.408185  -1.209045  1.095634  ...  2.690808  1.503688   \n",
      "4674  -7.611991 -3.427045  -8.350808  6.863604  ...  0.554027  0.832398   \n",
      "4638 -13.565130 -2.782438 -14.263735  9.643419  ...  1.243517 -1.614660   \n",
      "...         ...       ...        ...       ...  ...       ...       ...   \n",
      "1450   0.521847 -1.741244   0.019423  0.381790  ...  1.011011  0.205208   \n",
      "3566   0.122376 -0.323248   1.210819  0.105585  ... -2.101730 -0.324048   \n",
      "171    0.635981 -0.866537   1.138067 -0.924605  ...  0.100918 -0.431083   \n",
      "2050  -0.504798 -0.829963   0.177455 -0.181344  ... -0.137654 -0.133181   \n",
      "2046   2.046239  3.397838  -0.215105  1.427061  ...  0.222344  0.282483   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4778  0.325575  0.014002  0.844946  0.114963  0.156365 -0.619437 -0.120351   \n",
      "4882  0.262325 -0.431790 -0.092088  0.145216  0.457788  0.167739  0.451243   \n",
      "4730  0.838760  0.341727  0.947506 -0.145493  0.049326  0.831065  0.332421   \n",
      "4674  0.931958 -0.874467 -0.192639 -0.035426  0.538665 -0.263934  1.134095   \n",
      "4638  2.714045 -0.101355 -0.439666  0.519514  0.789328  0.064357 -1.621386   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1450  0.189500  0.234031 -0.202100  0.041873 -0.080313 -0.137458  0.245276   \n",
      "3566  0.105867  0.436059 -0.240564  0.170701  0.076297 -0.732521 -0.354709   \n",
      "171  -0.482119 -0.182518 -0.614101  0.037650  0.793080 -0.724708 -0.616486   \n",
      "2050 -0.097674 -0.115690 -0.066010  0.734489  0.688098 -0.364597  0.021047   \n",
      "2046 -0.274026 -0.706508 -0.130367  1.026032  0.297645  0.404274  0.372825   \n",
      "\n",
      "           V28  \n",
      "4778  0.035594  \n",
      "4882  0.268421  \n",
      "4730  0.252713  \n",
      "4674  0.225973  \n",
      "4638 -1.104819  \n",
      "...        ...  \n",
      "1450 -0.024124  \n",
      "3566 -0.376536  \n",
      "171  -0.485985  \n",
      "2050  0.025677  \n",
      "2046  0.214961  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.6751e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.6357e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 5.0089e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5833e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.3074e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0850e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0463e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.8731e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6645e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.5551e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.5551e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4626       1.253080    17.179453 -8.426814  6.241659 -9.946470  8.199614   \n",
      "2863      -0.239562     0.156075 -0.322984  0.711644  0.896844 -2.152729   \n",
      "2459      -0.085729    -0.019537 -0.836395  1.222372  1.589788  0.005322   \n",
      "528        1.151437    -0.737789 -2.153491  0.104021  1.459532 -1.613284   \n",
      "1186       0.833333    -0.504226 -0.658874  0.994656  1.621280  3.371777   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "3515       0.171458     0.409834 -1.311789  1.778326  1.033075  2.767400   \n",
      "4265       0.172142     0.738668 -1.320585 -0.991043  0.633928 -2.481152   \n",
      "2653      -0.222793     0.051147  0.076641  0.840775  0.630554  0.433788   \n",
      "1717      -0.017283    -0.325102 -0.695350 -0.973853  2.452479 -0.676376   \n",
      "185        0.427789    -0.855888 -0.505728  0.666300  1.080993 -0.447630   \n",
      "\n",
      "            V5        V6         V7        V8  ...       V20       V21  \\\n",
      "4626 -8.213093 -2.522046 -11.643028  5.339500  ...  0.563869  2.427460   \n",
      "2863  0.429934 -1.215491   1.174949 -0.421007  ...  0.057245  0.029648   \n",
      "2459 -0.198704 -1.074656   0.716620 -0.235059  ...  0.058887 -0.153673   \n",
      "528  -1.071361  0.192161  -0.332748  0.462717  ...  0.227422 -0.101937   \n",
      "1186  0.151203 -0.184932   0.655278 -0.291056  ...  0.061760 -0.048308   \n",
      "...        ...       ...        ...       ...  ...       ...       ...   \n",
      "3515  0.368220  0.045334   0.705935  0.214069  ... -0.149862  0.122470   \n",
      "4265  3.081888  2.647272  -1.067603  0.906425  ...  0.492035 -0.146948   \n",
      "2653  0.015145  0.348943  -0.587464 -1.402942  ... -0.350245  1.316097   \n",
      "1717 -0.126656 -0.240814  -0.878516 -0.105118  ...  0.191317  0.453279   \n",
      "185   1.039490  0.184017   0.882797 -0.142340  ...  0.435055 -0.058792   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "4626  0.692667  0.020305  0.499809  0.467594  0.483162  1.195671  0.198294   \n",
      "2863  0.476991 -0.359698 -0.104292  0.145480 -0.809105  0.239337 -0.018368   \n",
      "2459 -0.495737  0.010515  0.708366 -0.256932  0.010587 -0.136191  0.136858   \n",
      "528   0.096735 -0.490325 -0.392662  0.608578 -0.338382 -0.352132 -0.422726   \n",
      "1186  0.266926 -0.173916  0.649605 -0.456715  0.169096 -0.194693  0.014633   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3515  0.311254 -0.082732  0.116752  0.415781  0.357548 -0.395629 -0.023097   \n",
      "4265 -0.909814  0.060099  0.899750  0.435219 -0.589154 -0.043979  0.073881   \n",
      "2653 -1.191293  0.051294 -0.378302  0.228462  0.246281  0.209029  0.242257   \n",
      "1717  1.585425 -0.303820  0.263144 -0.638176 -0.035985 -0.180979 -0.324253   \n",
      "185   0.022073 -0.226241 -0.986435 -0.065937  0.259685  0.150573 -0.042581   \n",
      "\n",
      "      Class  \n",
      "4626      1  \n",
      "2863      0  \n",
      "2459      0  \n",
      "528       0  \n",
      "1186      0  \n",
      "...     ...  \n",
      "3515      0  \n",
      "4265      0  \n",
      "2653      0  \n",
      "1717      0  \n",
      "185       0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1         V2         V3         V4  \\\n",
      "4626       1.253080    17.179453  -8.426814   6.241659  -9.946470   8.199614   \n",
      "4777      -0.046030    36.273296  -4.868108   1.264420  -5.167885   3.193648   \n",
      "4521      -0.239562     2.641861  -1.783229   3.402794  -3.822742   2.625368   \n",
      "4836      -0.108830    41.729119 -17.976266  12.864989 -19.575066  11.345120   \n",
      "4849      -0.217659    44.109099  -1.456876   3.740306  -7.404518   7.440964   \n",
      "...             ...          ...        ...        ...        ...        ...   \n",
      "3361      -0.239562     0.367687  -0.300381   0.570978   0.716708  -2.214918   \n",
      "3441      -0.088809     0.379980  -5.151104   2.977071  -0.859917  -0.739685   \n",
      "1537       2.481177    -0.379541   0.952131  -0.660004  -0.351173  -1.408060   \n",
      "3541       0.940965     0.419054  -1.370946   0.593587   1.842636   0.223817   \n",
      "3683       1.966119     0.470859   0.922537  -0.517835   0.832344   0.913536   \n",
      "\n",
      "             V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4626  -8.213093 -2.522046 -11.643028  5.339500  ...  3.204309  0.563869   \n",
      "4777  -3.045621 -2.096166  -6.445610  2.422536  ...  1.385610  0.667310   \n",
      "4521  -1.976415 -2.731689  -3.430559  1.413204  ... -0.265422  0.364089   \n",
      "4836 -13.998646 -0.354900 -23.783470 -4.872353  ...  1.661029  2.419921   \n",
      "4849  -1.549878 -1.661697  -5.757213  1.615011  ...  0.065317  0.529557   \n",
      "...         ...       ...        ...       ...  ...       ...       ...   \n",
      "3361   0.433500 -1.064170   1.044465 -0.296050  ...  0.225710 -0.048111   \n",
      "3441   0.573801  2.065386  -1.264569 -4.303179  ... -1.662652 -0.660260   \n",
      "1537  -0.486406 -0.962104   0.341565 -0.203722  ...  0.624917  0.224722   \n",
      "3541  -0.502081 -0.092788   0.231925  0.210382  ... -0.445934  0.301482   \n",
      "3683  -0.826167  0.111915  -0.354734  0.082614  ... -0.463821  0.140578   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4626  2.427460  0.692667  0.020305  0.499809  0.467594  0.483162  1.195671   \n",
      "4777  1.269205  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274   \n",
      "4521  0.454032 -0.577526  0.045967  0.461700  0.044146  0.305704  0.530981   \n",
      "4836 -4.300432  2.865772  1.489302  0.386039 -0.232392  0.413827 -3.255981   \n",
      "4849  0.957897  0.145339 -0.044704 -0.544962 -0.757757 -0.005352  0.318152   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3361 -0.013581  0.265651 -0.379911 -0.494418  0.129384 -0.781942  0.227018   \n",
      "3441  3.884687 -1.893838  1.277402 -1.635821  0.344465  0.319386  1.223308   \n",
      "1537  0.153141  0.237123 -0.210545  0.298511  0.529161 -0.004023 -0.034051   \n",
      "3541  0.088592  0.794873 -0.083819  0.479359 -0.093705  0.475409  0.582144   \n",
      "3683  0.167155  0.493199 -0.187079  0.174856  0.390691  0.524809  0.001312   \n",
      "\n",
      "           V28  \n",
      "4626  0.198294  \n",
      "4777 -0.127944  \n",
      "4521  0.243746  \n",
      "4836 -0.538963  \n",
      "4849 -0.323554  \n",
      "...        ...  \n",
      "3361 -0.025715  \n",
      "3441  0.940238  \n",
      "1537  0.021217  \n",
      "3541  0.436372  \n",
      "3683  0.037852  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.2611e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9674e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.0281e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.9746e-04 - accuracy: 1.0000 - val_loss: 2.9976e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.4698e-04 - accuracy: 1.0000 - val_loss: 3.1248e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.9130e-04 - accuracy: 1.0000 - val_loss: 2.9960e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.4743e-04 - accuracy: 1.0000 - val_loss: 2.8923e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.1435e-04 - accuracy: 1.0000 - val_loss: 2.8421e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 7.8664e-04 - accuracy: 1.0000 - val_loss: 2.7896e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 7.4893e-04 - accuracy: 1.0000 - val_loss: 2.7267e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7267e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "3362      -0.239562     0.368126 -0.540909  0.485743  0.646909 -2.256652   \n",
      "2305      -0.137064    -0.097245  1.204461 -0.084453  0.479036  0.703425   \n",
      "970        0.176249    -0.586763  1.444837 -1.245312  0.776082 -1.491727   \n",
      "1270       0.002225    -0.480079 -0.595254  0.993109  1.788433  2.066058   \n",
      "1615      -0.239562    -0.358468 -2.594704  0.792116 -0.749193 -2.073082   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "1226       0.237851    -0.494128  1.130882 -0.306948  0.998749  0.225915   \n",
      "4848       1.522930    44.085830 -2.296987  4.064043 -5.957706  4.680008   \n",
      "1368      -0.239733    -0.442322  1.313171  0.078188 -1.291722 -0.483959   \n",
      "4474       0.351300     0.746570  1.094368  0.148772  1.171742  1.309366   \n",
      "4207       1.364648     0.736911  1.299369 -0.852354 -0.974292 -1.573454   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "3362  0.671964 -0.626104  0.942382 -0.196446  ... -0.100149  0.027194   \n",
      "2305 -0.456606 -0.096802 -0.268881  0.089622  ... -0.128189 -0.063122   \n",
      "970  -1.765425 -0.270934 -1.443537  0.108861  ... -0.333260 -0.211910   \n",
      "1270  0.689541  2.173362 -0.055776  0.626694  ... -0.028686 -0.273816   \n",
      "1615 -0.938225 -1.819945 -0.102909  1.103437  ...  0.070067  0.173200   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "1226 -0.544978  0.872664 -0.938950  0.372366  ...  0.024383  0.261644   \n",
      "4848 -2.080938 -1.463272 -4.490847  1.029246  ...  1.264954  1.089084   \n",
      "1368  2.140233  3.181208 -0.506359  0.805117  ...  0.030718 -0.343276   \n",
      "4474 -0.533612  0.028375 -0.505245 -0.014002  ... -0.072348  0.103551   \n",
      "4207  1.406209  3.293744 -1.100331  0.696654  ...  0.329556 -0.246124   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "3362  0.393594 -0.256605 -0.806563  0.069317 -0.761794  0.119072 -0.041058   \n",
      "2305  0.010919 -0.136416  0.065784  0.585370  0.442097 -0.022990 -0.003397   \n",
      "970  -0.336079  0.071196 -0.055183  0.134178 -0.291773  0.043139  0.018885   \n",
      "1270 -0.394840 -0.301300 -1.411518  0.045571 -0.019724  0.012914  0.067788   \n",
      "1615  0.377772  0.148204  0.587936  0.071497 -0.153507  0.262471 -0.081060   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1226  0.792110 -0.145284 -0.763391  0.212113  0.638136  0.024789  0.013678   \n",
      "4848  0.975398 -0.625530 -0.535181  0.247435  0.160400  0.969582  0.335041   \n",
      "1368 -1.147981  0.096151  0.946070  0.370076  0.110731 -0.021650  0.023466   \n",
      "4474  0.625461 -0.084649  0.061918  0.409214 -0.276609  0.047594  0.033268   \n",
      "4207 -0.926119  0.034717  0.922730  0.337647 -0.514011 -0.029904  0.024632   \n",
      "\n",
      "      Class  \n",
      "3362      0  \n",
      "2305      0  \n",
      "970       0  \n",
      "1270      0  \n",
      "1615      0  \n",
      "...     ...  \n",
      "1226      0  \n",
      "4848      1  \n",
      "1368      0  \n",
      "4474      0  \n",
      "4207      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3         V4  \\\n",
      "4660      -0.239562    19.083964  -1.519244  2.308492  -1.503599   2.064101   \n",
      "4745      -0.243669    30.185710   0.196707  1.189757   0.704882   2.891388   \n",
      "4675       1.454312    21.539897 -10.527304  7.639745 -13.443115   4.303403   \n",
      "4709       5.205339    25.333992  -8.257111 -4.814461  -5.365307   1.204230   \n",
      "4804      -0.256502    40.286028  -3.765680  5.890735 -10.202268  10.259036   \n",
      "...             ...          ...        ...       ...        ...        ...   \n",
      "1448      -0.077687    -0.416420  -1.102491  1.355435   2.118981   0.242654   \n",
      "83         1.269165    -0.886621  -1.198968 -1.474100   1.840326  -4.515824   \n",
      "2093      -0.239562    -0.201295  -2.962376  2.607363  -0.067883  -0.889316   \n",
      "2720       0.924025     0.084513  -1.266171 -0.321459   1.784605  -2.404879   \n",
      "2473      -0.241444    -0.014707  -4.454186 -3.581156  -0.522735   2.216885   \n",
      "\n",
      "            V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4660 -1.000845 -1.016897  -2.059731 -0.275166  ...  1.160120  0.175019   \n",
      "4745  0.045555  1.245730  -1.198714 -2.421616  ...  0.329827  0.646616   \n",
      "4675 -8.048210 -3.466997  -8.643193  7.284105  ...  0.511762  0.847085   \n",
      "4709 -3.347420 -1.331601  -1.967893  1.295438  ...  0.105879 -1.233987   \n",
      "4804 -5.611448 -3.235376 -10.632683  3.272716  ...  2.500827  1.194137   \n",
      "...        ...       ...        ...       ...  ...       ...       ...   \n",
      "1448  0.685721  0.468232   1.130935 -0.675945  ...  1.119734  0.439737   \n",
      "83    0.327567 -0.174469   0.959726 -1.026456  ... -1.512022 -0.769854   \n",
      "2093 -1.360313 -0.983393  -0.534070  1.473261  ... -0.202505  0.420581   \n",
      "2720 -1.251838 -0.348121  -0.675538  0.489034  ... -1.356222 -0.015154   \n",
      "2473  6.077260 -5.500163  -1.928049 -0.207121  ... -1.434199  0.155928   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4660  1.307871  0.102826 -0.017746  0.149696 -0.096602 -0.369115 -0.019244   \n",
      "4745 -1.328132  0.189311 -0.005524 -0.814708  0.400924  0.286281  0.135215   \n",
      "4675  0.937416 -0.931178 -0.235697 -0.031393  0.591558 -0.263516  1.108897   \n",
      "4709  0.436390 -0.077553 -3.091624 -0.390201 -0.288689 -0.340004  0.039819   \n",
      "4804  2.245606  0.546321  0.381853  0.382025 -0.821036  0.394355  1.412961   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1448 -0.310349 -0.284566 -0.399066 -0.519518  0.134754 -0.644481 -1.273598   \n",
      "83   -0.453169  0.334614 -0.364541 -0.310186 -0.302599 -1.243924 -1.123457   \n",
      "2093 -0.185342 -0.455737  0.160595  0.506340  0.078372  0.299425  0.544370   \n",
      "2720  0.085012  0.446617 -0.090068  0.095560  0.434890 -0.147216  0.196593   \n",
      "2473  0.036795 -0.215103  0.096331  0.669872 -0.168050 -0.595656  0.045526   \n",
      "\n",
      "           V28  \n",
      "4660 -0.208319  \n",
      "4745  0.257315  \n",
      "4675  0.219021  \n",
      "4709 -1.007900  \n",
      "4804  0.782407  \n",
      "...        ...  \n",
      "1448 -1.057595  \n",
      "83   -0.734351  \n",
      "2093  0.349456  \n",
      "2720  0.023946  \n",
      "2473  0.851741  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.5942e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 8.9903e-04 - accuracy: 1.0000 - val_loss: 3.5621e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 7.9849e-04 - accuracy: 1.0000 - val_loss: 3.5862e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 7.4796e-04 - accuracy: 1.0000 - val_loss: 3.5803e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 7.0281e-04 - accuracy: 1.0000 - val_loss: 3.4087e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 6.6988e-04 - accuracy: 1.0000 - val_loss: 3.1974e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 6.4297e-04 - accuracy: 1.0000 - val_loss: 3.1883e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 6.1323e-04 - accuracy: 1.0000 - val_loss: 3.1874e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 5.9812e-04 - accuracy: 1.0000 - val_loss: 3.0841e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 5.6961e-04 - accuracy: 1.0000 - val_loss: 2.9550e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9550e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "3165       0.910849     0.293930 -1.032672  0.288381  2.063876  0.353579   \n",
      "1880      -0.195243    -0.273296 -0.669051  1.137969  1.169320 -0.147124   \n",
      "4649      -0.239562    17.637800 -2.524012  2.098152 -4.946075  6.456588   \n",
      "795        2.048255    -0.644276  0.888448 -0.629868  1.344220  0.715795   \n",
      "974       -0.085729    -0.585446  1.182742  0.235351  0.389091  0.638299   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "4479      -0.256674     0.750960  1.106262  0.398131  1.815655  2.990708   \n",
      "2704      -0.239562     0.076611 -1.363919  0.247680  0.909595  1.127503   \n",
      "4335      -0.074949     0.741302  1.017648 -0.043059  0.684992  0.913154   \n",
      "1494       2.446954    -0.398859  0.935047 -0.584762  0.196066  0.447476   \n",
      "4328       0.316222     0.740863  1.417736 -0.656760 -0.905589 -1.623224   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "3165 -0.298921 -0.365203  0.400751  0.095128  ... -0.209219  0.192060   \n",
      "1880  0.196541 -0.234215  0.461928  0.258223  ...  0.176081 -0.223611   \n",
      "4649  3.173921 -3.058806 -0.184710 -0.390420  ... -0.162797  0.027935   \n",
      "795  -1.449115 -0.188218 -0.766771  0.206372  ...  0.155566  0.359282   \n",
      "974  -0.479632 -0.877410 -0.045612 -0.034499  ... -0.101705 -0.225136   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "4479 -0.865004  0.009125 -0.626067  0.048107  ... -0.202821 -0.212720   \n",
      "2704  1.555518 -0.807158  1.390296 -1.017523  ... -0.371190 -0.357274   \n",
      "4335  0.133533  1.170727 -0.460130  0.378469  ... -0.324884 -0.242925   \n",
      "1494 -0.601640 -0.236399  0.000171  0.014678  ...  0.182982 -0.126611   \n",
      "4328  1.466636  3.271695 -1.181635  0.712557  ...  0.204782 -0.283250   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "3165  0.599978 -0.164235  0.386969  0.151719 -0.396472  0.003126  0.166778   \n",
      "1880 -0.536028  0.014242 -0.053043 -0.233447  0.091225  0.364371  0.142858   \n",
      "4649  0.220366  0.976348 -0.290539  1.161002  0.663954  0.456023 -0.405682   \n",
      "795   0.736915 -0.127126  0.573479  0.085966  0.429654 -0.009815  0.045354   \n",
      "974  -0.738237  0.142697  0.477843  0.133592  0.065680 -0.032476  0.022722   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4479 -0.199560  0.094663  0.646047  0.238918 -0.102657  0.018308  0.033028   \n",
      "2704  0.483963  0.470293 -0.082912 -0.230531 -0.388476 -0.439026 -0.779125   \n",
      "4335 -0.144215  0.192951 -0.674396  0.059524  0.339840  0.019932 -0.005076   \n",
      "1494 -0.588298 -0.156607  0.053381  0.343463  0.490779 -0.076862  0.020418   \n",
      "4328 -0.893782  0.088720  0.919586  0.360144 -0.507477 -0.018327  0.013964   \n",
      "\n",
      "      Class  \n",
      "3165      0  \n",
      "1880      0  \n",
      "4649      1  \n",
      "795       0  \n",
      "974       0  \n",
      "...     ...  \n",
      "4479      0  \n",
      "2704      0  \n",
      "4335      0  \n",
      "1494      0  \n",
      "4328      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4649      -0.239562    17.637800 -2.524012  2.098152 -4.946075  6.456588   \n",
      "4768       0.770021    33.652727 -4.312479  1.886476 -2.338634 -0.475243   \n",
      "4781      -0.256674    36.532763 -7.030308  3.421991 -9.525072  5.270891   \n",
      "4517      -0.239562     2.431127  0.725646  2.300894 -5.329976  4.007683   \n",
      "4783       4.071184    36.532763 -6.713407  3.921104 -9.746678  5.148263   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "2964      -0.243669     0.191636  1.335053  0.331464 -2.057763 -0.346175   \n",
      "3168       0.168549     0.295248  1.110541  0.212428  0.537610  1.476218   \n",
      "680        0.369781    -0.684228 -0.878340  0.489031  2.914341  0.078154   \n",
      "1385       2.446954    -0.437054 -2.095310  0.372403  0.186651 -0.440886   \n",
      "605       -0.205510    -0.708374  1.294157  1.132084 -2.148605  1.230031   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V19       V20  \\\n",
      "4649  3.173921 -3.058806 -0.184710 -0.390420  ... -1.063028 -0.162797   \n",
      "4768 -1.185444 -2.112079 -2.122793  0.272565  ... -0.009534 -0.718706   \n",
      "4781 -4.024630 -2.865682 -6.989195  3.791551  ...  0.876019  0.545698   \n",
      "4517 -1.730411 -1.732193 -3.968593  1.063728  ...  2.250123  0.504646   \n",
      "4783 -5.151563 -2.099389 -5.937767  3.578780  ...  0.897402  0.135711   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "2964  2.583234  2.854102 -0.187547  0.685154  ...  0.049350  0.045217   \n",
      "3168 -0.262395 -0.344631  0.142262 -0.078189  ... -0.500202 -0.124489   \n",
      "680  -0.423732  0.090021  0.289963  0.085451  ...  0.385202  0.363765   \n",
      "1385 -0.609495 -0.073792  0.923225  0.323543  ...  0.276185 -0.463425   \n",
      "605   1.447894 -0.767676  0.575635 -0.094475  ... -0.038914 -0.046194   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4649  0.027935  0.220366  0.976348 -0.290539  1.161002  0.663954  0.456023   \n",
      "4768  0.550541 -0.067870 -1.114692  0.269069 -0.020572 -0.963489 -0.918888   \n",
      "4781  1.103398 -0.541855  0.036943 -0.355519  0.353634  1.042458  1.359516   \n",
      "4517  0.589669  0.109541  0.601045 -0.364700 -1.843078  0.351909  0.594550   \n",
      "4783  0.954272 -0.451086  0.127214 -0.339450  0.394096  1.075295  1.649906   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2964 -0.191820 -0.650118 -0.114069  0.915936  0.730073  0.383879 -0.031902   \n",
      "3168 -0.062755  0.049586 -0.044224  0.440124  0.636320 -0.336996  0.041561   \n",
      "680  -0.037353  0.236372 -0.197062  0.584250  0.190462  0.305561  0.122975   \n",
      "1385 -0.209954 -0.373133  0.178694 -0.383664  0.014457  0.334662 -0.639994   \n",
      "605  -0.257463 -0.726440 -0.374828 -1.358994  0.968892 -0.225584  0.027084   \n",
      "\n",
      "           V28  \n",
      "4649 -0.405682  \n",
      "4768  0.001454  \n",
      "4781 -0.272188  \n",
      "4517  0.099372  \n",
      "4783 -0.394905  \n",
      "...        ...  \n",
      "2964  0.029849  \n",
      "3168  0.023643  \n",
      "680  -0.088435  \n",
      "1385 -0.252820  \n",
      "605   0.071322  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.1033e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 9.0313e-04 - accuracy: 1.0000 - val_loss: 9.0541e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 8.2249e-04 - accuracy: 1.0000 - val_loss: 9.1677e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 7.4681e-04 - accuracy: 1.0000 - val_loss: 1.0072e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 7.0695e-04 - accuracy: 1.0000 - val_loss: 1.0358e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 6.6499e-04 - accuracy: 1.0000 - val_loss: 1.0368e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 6.4412e-04 - accuracy: 1.0000 - val_loss: 1.0354e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 5.9759e-04 - accuracy: 1.0000 - val_loss: 1.0086e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 5.9120e-04 - accuracy: 1.0000 - val_loss: 1.0034e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 5.6266e-04 - accuracy: 1.0000 - val_loss: 1.0289e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0289e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "824        0.729637    -0.632861 -1.058931  0.114078  1.485337  0.534704   \n",
      "4777      -0.046030    36.273296 -4.868108  1.264420 -5.167885  3.193648   \n",
      "340       -0.154689    -0.800132 -1.337375  1.109391  1.396223  0.038718   \n",
      "4933      -0.229466    64.089782  1.261324  2.726800 -5.435019  5.342759   \n",
      "742        0.045859    -0.664911 -0.439880  0.630451  1.942591 -0.798747   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "955        2.820671    -0.592471 -2.240566  0.466023  1.566136  1.158490   \n",
      "2856      -0.239562     0.154758 -0.323085  0.701661  0.893979 -2.151390   \n",
      "4289       2.277379     0.739546  1.196322 -1.022632 -1.034102 -1.530126   \n",
      "4059       0.544148     0.723741  1.026189 -0.074026  1.497202  1.509733   \n",
      "1563       0.151951    -0.371200 -1.113706 -0.290950  1.117998 -0.159200   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "824   0.754275 -0.496412  0.580912  0.143711  ... -0.014561  0.067737   \n",
      "4777 -3.045621 -2.096166 -6.445610  2.422536  ...  0.667310  1.269205   \n",
      "340   0.520267 -0.259157  0.806329 -0.016192  ...  0.137318 -0.200431   \n",
      "4933  1.447043 -1.442584 -0.898702  0.123062  ...  0.313332  0.209086   \n",
      "742   0.261325 -0.617939  1.224288 -0.844641  ...  0.129803 -0.085056   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "955  -1.620135  0.743998  0.225636  0.720140  ... -0.423730  0.048145   \n",
      "2856  0.417636 -1.218769  1.165600 -0.413178  ...  0.048417  0.028063   \n",
      "4289  1.353603  3.312939 -1.029550  0.682809  ...  0.438181 -0.213804   \n",
      "4059 -0.979557  0.063401 -0.692738  0.126376  ... -0.089563 -0.123622   \n",
      "1563  1.427171 -0.856862  0.199476 -0.340134  ... -0.464608 -0.323045   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "824   0.056591  0.032287  0.035178  0.098192 -0.489207  0.090101  0.137386   \n",
      "4777  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274 -0.127944   \n",
      "340  -0.116354 -0.398652 -0.034692  0.444578 -0.489118  0.259927  0.065547   \n",
      "4933 -0.425938 -0.154440 -0.018820  0.632234  0.192922  0.468181  0.280486   \n",
      "742   0.177107 -0.142910  0.466897 -0.438533  0.803909 -0.907362 -0.614642   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "955   0.310025  0.109136  0.072415  0.178581 -0.283594 -0.559090 -0.286753   \n",
      "2856  0.465706 -0.358044 -0.108064  0.140817 -0.808643  0.238436 -0.018664   \n",
      "4289 -0.954272 -0.012297  0.925467  0.318062 -0.519700 -0.039983  0.033919   \n",
      "4059 -0.018446 -0.014677  0.477498  0.324373 -0.481738  0.035984  0.030464   \n",
      "1563 -0.418547 -0.487299 -0.387544 -0.215703  0.062328 -0.181186 -0.240170   \n",
      "\n",
      "      Class  \n",
      "824       0  \n",
      "4777      1  \n",
      "340       0  \n",
      "4933      1  \n",
      "742       0  \n",
      "...     ...  \n",
      "955       0  \n",
      "2856      0  \n",
      "4289      0  \n",
      "4059      0  \n",
      "1563      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "4777      -0.046030    36.273296 -4.868108  1.264420 -5.167885  3.193648   \n",
      "4933      -0.229466    64.089782  1.261324  2.726800 -5.435019  5.342759   \n",
      "4720       4.789528    27.228844 -6.133987  2.941499 -5.593986  3.258845   \n",
      "4942       0.040897    64.802327  1.833191  0.745333 -1.133009  3.893556   \n",
      "4539       0.263347     4.400176 -4.727713  3.044469 -5.598354  5.928191   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "1480       0.047741    -0.401493  1.150559  0.217028  0.503593  1.066363   \n",
      "1380       2.446954    -0.438371 -2.248488 -0.195670  0.026949 -0.525780   \n",
      "2377       2.922998    -0.062123 -1.161086 -0.079347  1.010443  0.243638   \n",
      "2323       0.052361    -0.090221  1.163942 -0.409313 -0.353590  0.600512   \n",
      "1297       0.307495    -0.462957  1.293271 -0.686658  0.763193 -0.750240   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V19       V20  \\\n",
      "4777 -3.045621 -2.096166 -6.445610  2.422536  ...  1.385610  0.667310   \n",
      "4933  1.447043 -1.442584 -0.898702  0.123062  ... -1.466911  0.313332   \n",
      "4720 -5.315512 -0.637328 -4.476488  1.695994  ...  1.763611 -0.815086   \n",
      "4942  0.858164  0.910235 -0.498200  0.344703  ... -1.616927 -0.085579   \n",
      "4539 -2.190770 -1.529323 -4.487422  0.916392  ...  1.504925 -0.207759   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "1480 -0.461525 -0.745174  0.041322 -0.021768  ... -0.063033 -0.167924   \n",
      "1380 -0.238759 -0.033359  0.162196  0.928520  ...  0.516124 -0.082562   \n",
      "2377  0.848064  0.524867  1.104558  0.186342  ... -0.454843  0.360198   \n",
      "2323  1.614795  4.388608 -1.034458  1.149803  ...  0.903703 -0.045551   \n",
      "1297 -1.216752 -0.267433 -0.862741  0.037486  ... -0.480060 -0.360153   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4777  1.269205  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274   \n",
      "4933  0.209086 -0.425938 -0.154440 -0.018820  0.632234  0.192922  0.468181   \n",
      "4720  0.868340  0.793736  0.217347 -0.021985  0.145882  0.665088 -1.684186   \n",
      "4942  0.039289  0.181652  0.072981 -0.155299 -0.149891  0.012792  0.040854   \n",
      "4539  0.650988  0.254983  0.628843 -0.238128 -0.671332 -0.033590 -1.331777   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1480 -0.111975 -0.421642  0.057032  0.475331  0.380089 -0.580347  0.007138   \n",
      "1380 -0.032208 -0.745757 -0.243193 -0.821633 -0.030402  0.335838 -0.286998   \n",
      "2377  0.167278  0.199204  0.217637 -0.665093  0.185581 -0.504411  0.065101   \n",
      "2323 -0.493230 -1.151827  0.014007  0.995618  0.651431 -0.524167  0.075047   \n",
      "1297 -0.423452 -0.830487  0.123544  0.026530 -0.072321  0.855177 -0.036263   \n",
      "\n",
      "           V28  \n",
      "4777 -0.127944  \n",
      "4933  0.280486  \n",
      "4720  0.310195  \n",
      "4942  0.022903  \n",
      "4539  0.705698  \n",
      "...        ...  \n",
      "1480  0.018426  \n",
      "1380 -0.243768  \n",
      "2377  0.140659  \n",
      "2323  0.026361  \n",
      "1297  0.013303  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 1.0429e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 9.8504e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 1.2680e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 7.6276e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 6.1388e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 9.8345e-04 - accuracy: 1.0000 - val_loss: 5.9761e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 8.4643e-04 - accuracy: 1.0000 - val_loss: 5.7587e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 7.4012e-04 - accuracy: 1.0000 - val_loss: 5.5484e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 6.7578e-04 - accuracy: 1.0000 - val_loss: 5.3860e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 6.3122e-04 - accuracy: 1.0000 - val_loss: 5.2936e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 5.2936e-05 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time         V1         V2         V3        V4  \\\n",
      "85         3.165811    -0.885743  -4.575093  -4.429184   3.402585  0.903915   \n",
      "4758       4.102669    32.819010  -6.616293   3.563428  -7.058901  4.284346   \n",
      "1911       0.531828    -0.263637  -0.814123   0.727476   1.621366  0.727475   \n",
      "4885      -0.222450    56.018220   0.753356   2.284988  -5.164492  3.831112   \n",
      "2905      -0.085729     0.164856   1.220338  -0.019368   0.889428  1.110236   \n",
      "...             ...          ...        ...        ...        ...       ...   \n",
      "2295       0.169405    -0.102074   1.038611  -0.559941   1.651324  1.639150   \n",
      "1476      -0.239562    -0.403249   1.247541   1.251792  -1.602325  1.653127   \n",
      "4709       5.205339    25.333992  -8.257111  -4.814461  -5.365307  1.204230   \n",
      "4572       1.454312    10.870596 -20.532751  12.373989 -23.009003  6.144821   \n",
      "532        0.003251    -0.734716   1.091666   0.068682   1.230334  2.554474   \n",
      "\n",
      "             V5        V6         V7         V8  ...       V20       V21  \\\n",
      "85     3.002224 -0.491078  -2.705393   0.666451  ... -0.960963 -0.047365   \n",
      "4758  -5.096299 -1.768618  -4.937554   2.748460  ... -0.541634  1.215976   \n",
      "1911   0.563880 -0.278734   0.709642   0.020798  ...  0.263094  0.170308   \n",
      "4885  -0.073622 -1.316596  -1.855495   0.831079  ...  0.285792  0.382007   \n",
      "2905  -0.460948  0.368122  -0.543361   0.122383  ... -0.066684 -0.119578   \n",
      "...         ...       ...        ...        ...  ...       ...       ...   \n",
      "2295  -1.292460  0.985490  -1.207915   0.564181  ... -0.203912 -0.044650   \n",
      "1476   0.777110 -1.845478   0.729759  -0.347520  ... -0.080438 -0.218636   \n",
      "4709  -3.347420 -1.331601  -1.967893   1.295438  ... -1.233987  0.436390   \n",
      "4572 -15.587296 -4.384491 -15.939003  13.696416  ...  1.592754  1.754608   \n",
      "532   -0.454169  0.915781  -0.701016   0.396866  ... -0.216638 -0.098440   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "85    0.853360 -0.971600 -0.114862  0.408300 -0.304576  0.547785 -0.456297   \n",
      "4758  0.041178 -1.059098  0.275662  0.057425 -0.265838 -0.514637  0.388590   \n",
      "1911  0.328135 -0.286202  0.027621  0.570779 -0.195016  0.045185  0.071284   \n",
      "4885  0.033958  0.187697  0.358433 -0.488934 -0.258802  0.296145 -0.047174   \n",
      "2905 -0.107971 -0.131100 -0.560655  0.562442 -0.350990  0.074617  0.029776   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2295  0.242826 -0.145980  0.020555  0.518297 -0.219250  0.092627  0.025117   \n",
      "1476 -0.562316 -0.200743  0.036246  0.834931 -0.291626  0.035468  0.095443   \n",
      "4709 -0.077553 -3.091624 -0.390201 -0.288689 -0.340004  0.039819 -1.007900   \n",
      "4572 -1.466115 -0.856779  0.125777  1.402587 -0.223755  1.574249  0.469201   \n",
      "532  -0.178428  0.007277 -0.504025  0.255974  0.022213  0.048714  0.027057   \n",
      "\n",
      "      Class  \n",
      "85        0  \n",
      "4758      1  \n",
      "1911      0  \n",
      "4885      1  \n",
      "2905      0  \n",
      "...     ...  \n",
      "2295      0  \n",
      "1476      0  \n",
      "4709      1  \n",
      "4572      1  \n",
      "532       0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3        V4  \\\n",
      "4758       4.102669    32.819010  -6.616293  3.563428  -7.058901  4.284346   \n",
      "4885      -0.222450    56.018220   0.753356  2.284988  -5.164492  3.831112   \n",
      "4896      15.576831    58.257710  -0.967767  2.098019  -5.222929  6.514573   \n",
      "4676       1.454312    21.548677 -11.205461  7.914633 -13.987752  4.333341   \n",
      "4756       1.452772    32.446713  -5.140723  3.568751  -5.896245  4.164720   \n",
      "...             ...          ...        ...       ...        ...       ...   \n",
      "2520      -0.239562     0.008561  -0.278959  0.646643   1.220160 -0.892089   \n",
      "3045      -0.210643     0.235539  -0.468587  1.079763   1.693916 -0.110240   \n",
      "3888       0.045859     0.608276   1.152838 -0.159998   0.667810 -0.166714   \n",
      "3873      -0.181383     0.593349  -1.844120  1.493511   1.414584  1.255638   \n",
      "7          0.441478    -0.906816  -0.644269  1.417964   1.074380 -0.492199   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V19       V20  \\\n",
      "4758 -5.096299 -1.768618 -4.937554  2.748460  ...  2.143502 -0.541634   \n",
      "4885 -0.073622 -1.316596 -1.855495  0.831079  ...  2.362291  0.285792   \n",
      "4896 -4.187674  2.114178  0.948701 -2.448427  ...  0.952141  1.147204   \n",
      "4676 -8.484970 -3.506561 -8.935243  7.704449  ...  0.469554  0.860912   \n",
      "4756 -4.091193 -1.989960 -5.472436  2.422821  ...  1.737976  0.318995   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "2520  0.019214 -0.985625  0.629303 -0.178905  ...  1.168707  0.086175   \n",
      "3045 -0.019649 -0.664751  0.678538  0.007483  ...  0.219287  0.168495   \n",
      "3888 -0.655924 -0.346184 -0.356477  0.095971  ... -0.123230 -0.055579   \n",
      "3873  0.051745 -0.541168  0.805225 -0.686486  ...  1.542782 -0.103092   \n",
      "7     0.948934  0.428118  1.120631 -3.807864  ...  0.324505 -0.156742   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4758  1.215976  0.041178 -1.059098  0.275662  0.057425 -0.265838 -0.514637   \n",
      "4885  0.382007  0.033958  0.187697  0.358433 -0.488934 -0.258802  0.296145   \n",
      "4896 -0.843268  0.796739  1.314312 -0.352887 -1.770706  0.098133  0.956769   \n",
      "4676  0.942593 -0.987848 -0.279446 -0.027299  0.644344 -0.263078  1.084023   \n",
      "4756  1.131130  0.118022 -0.332704  0.139941  0.324758 -0.180769  0.177810   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2520  0.222747  0.847161 -0.186965  0.452004 -0.106871 -0.523168  0.206507   \n",
      "3045 -0.150869 -0.358060 -0.022338  0.545286 -0.215801  0.037574  0.268735   \n",
      "3888 -0.030664 -0.107994  0.124146  0.277474 -0.015418  0.902612 -0.060023   \n",
      "3873  0.052900  0.332851 -0.124460  0.400639  0.362232 -0.162329 -1.098938   \n",
      "7     1.943465 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921   \n",
      "\n",
      "           V28  \n",
      "4758  0.388590  \n",
      "4885 -0.047174  \n",
      "4896  0.162777  \n",
      "4676  0.211933  \n",
      "4756  0.661555  \n",
      "...        ...  \n",
      "2520  0.148543  \n",
      "3045  0.115127  \n",
      "3888  0.000707  \n",
      "3873 -0.062638  \n",
      "7    -1.085339  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 8.4880e-04 - accuracy: 1.0000 - val_loss: 7.6831e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 7.1908e-04 - accuracy: 1.0000 - val_loss: 6.9944e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 6.4662e-04 - accuracy: 1.0000 - val_loss: 6.6218e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 6.0138e-04 - accuracy: 1.0000 - val_loss: 6.4656e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 5.6468e-04 - accuracy: 1.0000 - val_loss: 6.1772e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 5.2465e-04 - accuracy: 1.0000 - val_loss: 6.2310e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 5.0142e-04 - accuracy: 1.0000 - val_loss: 6.0683e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4.6910e-04 - accuracy: 1.0000 - val_loss: 5.7185e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4.5366e-04 - accuracy: 1.0000 - val_loss: 5.7050e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4.2821e-04 - accuracy: 1.0000 - val_loss: 5.3656e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.3656e-05 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n",
      "      scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
      "3133       3.319644     0.279881 -0.733264  0.220740  2.154380  0.043069   \n",
      "3846      -0.057153     0.574031 -1.660254  1.896936 -0.035586  0.781848   \n",
      "2776      -0.241444     0.108221  0.154568  0.941367  1.417823  1.053450   \n",
      "552        0.727242    -0.728570  1.146594 -0.353208  0.834120  0.708139   \n",
      "76         0.769849    -0.888377 -0.549626  0.418949  1.729833  0.203065   \n",
      "...             ...          ...       ...       ...       ...       ...   \n",
      "1963       0.085387    -0.246954 -0.372594  0.291697  1.437615 -1.442014   \n",
      "2022       1.255989    -0.220173 -0.907342  0.744462  2.301932  1.517583   \n",
      "1454       0.961157    -0.414225 -0.403358  0.307439  1.886847  0.901240   \n",
      "1898      -0.171116    -0.265833 -1.629826  1.326529  0.906349 -0.800461   \n",
      "3619       1.779603     0.449347 -0.249964 -0.371521  1.410132 -1.126919   \n",
      "\n",
      "            V5        V6        V7        V8  ...       V20       V21  \\\n",
      "3133 -1.042791  0.842349  0.960767 -0.059609  ...  0.049009 -0.022857   \n",
      "3846 -0.598215 -0.402011 -0.368105  1.144671  ... -0.341362  0.368000   \n",
      "2776  0.039278 -0.717523  0.639908 -0.350254  ...  0.251350 -0.234494   \n",
      "552  -0.749014  0.189676 -0.593212  0.126902  ...  0.011619 -0.120062   \n",
      "76   -0.187012  0.253878  0.500894  0.251256  ...  0.016970  0.115062   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "1963  0.028033 -0.117853  0.434629  0.173717  ... -0.043671 -0.117484   \n",
      "2022 -0.726188  1.089493  0.496078  0.165447  ...  0.188409 -0.480747   \n",
      "1454 -0.332366 -0.261843  0.706182 -0.103885  ...  0.157012 -0.076218   \n",
      "1898 -0.283720 -0.383247  0.201000  0.585984  ...  0.080721 -0.197779   \n",
      "3619 -1.219167  0.878293 -0.493258  0.146646  ...  0.147753  0.499965   \n",
      "\n",
      "           V22       V23       V24       V25       V26       V27       V28  \\\n",
      "3133  0.219450 -0.091515  0.028696 -0.283616  0.236626 -0.098537 -0.166565   \n",
      "3846  0.539893 -0.094375  0.024265 -0.206283 -0.366751 -0.609295 -0.046866   \n",
      "2776 -0.451775  0.022609  0.362480 -0.656059  0.242236 -0.078416 -0.137300   \n",
      "552  -0.268422 -0.112320 -0.410470  0.341273  0.364006  0.005175  0.026810   \n",
      "76    0.418529 -0.065133  0.264981  0.003958  0.395969  0.027182  0.043506   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1963 -0.262198 -0.019567 -0.334041 -0.413197  0.734334  0.003457 -0.176634   \n",
      "2022 -0.554517 -0.074535  0.059955 -0.064549 -0.400035  0.300230 -0.002793   \n",
      "1454 -0.031594  0.154256  0.597104 -0.554755  0.311924 -0.057622 -0.077744   \n",
      "1898 -0.385304 -0.066693  0.022864  0.141284  0.385286  0.250911  0.192462   \n",
      "3619  1.472650 -0.040344 -0.785061 -0.361566 -0.005049 -0.075794  0.026576   \n",
      "\n",
      "      Class  \n",
      "3133      0  \n",
      "3846      0  \n",
      "2776      0  \n",
      "552       0  \n",
      "76        0  \n",
      "...     ...  \n",
      "1963      0  \n",
      "2022      0  \n",
      "1454      0  \n",
      "1898      0  \n",
      "3619      0  \n",
      "\n",
      "[3494 rows x 31 columns]\n",
      "Relation fraud non-fraud in the CV set: 343 x 343\n",
      "Test training relationship 686 x 6\n",
      "Test training relationship 0.9913294797687862 x 0.008670520231213872\n",
      "      scaled_amount  scaled_time         V1        V2         V3         V4  \\\n",
      "4553      -0.239562     7.167819 -14.474437  6.503185 -17.712632  11.270352   \n",
      "4886      -0.239562    56.050269  -1.396204  2.618584  -6.036770   3.552454   \n",
      "4812       2.969199    40.297443 -10.632375  7.251936 -17.681072   8.204144   \n",
      "4844       2.273614    43.694216   0.913116  1.145381  -4.602878   2.091803   \n",
      "4635      -0.243669    17.225552 -13.680760  6.990389 -13.770001   8.694897   \n",
      "...             ...          ...        ...       ...        ...        ...   \n",
      "3150       0.169405     0.287784   0.957784 -0.593668   1.849287   1.716378   \n",
      "3456       1.365503     0.381736  -0.455382  0.465230   1.478102   1.218583   \n",
      "602        1.822040    -0.709253  -0.469790 -0.009723   1.759079  -0.086506   \n",
      "2554      -0.239562     0.014268  -0.643331  0.876486   1.114308  -2.067565   \n",
      "3928       0.707050     0.641203  -0.551627  0.880860   1.960172   1.290978   \n",
      "\n",
      "             V5        V6         V7        V8  ...       V19       V20  \\\n",
      "4553  -4.150142 -3.372098 -16.535807 -1.443947  ...  0.579200  1.101250   \n",
      "4886   1.030091 -2.950358  -1.528506  0.189319  ...  0.170144  0.031795   \n",
      "4812 -10.166591 -4.510344 -12.981606  6.783589  ...  3.813041 -0.810146   \n",
      "4844  -0.473224 -2.085436  -1.671244  0.094397  ...  0.252274  0.508985   \n",
      "4635 -11.426968 -2.919845 -14.594562  8.622905  ...  3.517611 -0.877392   \n",
      "...         ...       ...        ...       ...  ...       ...       ...   \n",
      "3150  -1.369590  1.143323  -1.273025  0.666695  ...  0.034453 -0.253238   \n",
      "3456   0.393910  0.084723   1.075616 -0.220123  ...  0.415437  0.295604   \n",
      "602   -0.508940  0.757607   0.256744  0.208422  ...  1.084289  0.367977   \n",
      "2554   0.181702 -1.228307   1.117487 -0.333802  ... -0.615700  0.140372   \n",
      "3928   0.141388  0.048400   1.178933 -0.401107  ...  0.643690  0.237686   \n",
      "\n",
      "           V21       V22       V23       V24       V25       V26       V27  \\\n",
      "4553 -2.475962  0.342391 -3.564508 -0.818140  0.153408  0.755079  2.706566   \n",
      "4886  0.143177 -0.390176  0.356029 -0.762352  0.096510 -0.487861  0.062655   \n",
      "4812  2.715357  0.695603 -1.138122  0.459442  0.386337  0.522438 -1.416604   \n",
      "4844  0.401416 -0.084933 -0.197684 -0.283271  0.387597 -0.553527  0.516274   \n",
      "4635  2.989553  0.497599 -0.509290  0.732503  0.280528  0.280037 -1.406687   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3150  0.018127  0.456629 -0.051369  0.228856  0.374209 -0.221942  0.109926   \n",
      "3456  0.049360  0.392740  0.067609  0.106963 -0.240801 -0.352548  0.004406   \n",
      "602  -0.030373  0.050121  0.248683 -0.373362 -0.725709  0.992874  0.106658   \n",
      "2554  0.084476  0.735502 -0.218425  0.411130  0.019755 -0.826653  0.362879   \n",
      "3928 -0.138282  0.276980 -0.252771  0.411485  0.072293 -0.260608 -0.035620   \n",
      "\n",
      "           V28  \n",
      "4553 -0.992916  \n",
      "4886 -0.240732  \n",
      "4812 -0.488307  \n",
      "4844  0.270828  \n",
      "4635 -0.663643  \n",
      "...        ...  \n",
      "3150  0.026982  \n",
      "3456 -0.052381  \n",
      "602   0.160211  \n",
      "2554  0.047016  \n",
      "3928 -0.291230  \n",
      "\n",
      "[686 rows x 30 columns]\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 4.7602e-04 - accuracy: 1.0000 - val_loss: 3.2762e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 4.2672e-04 - accuracy: 1.0000 - val_loss: 3.0064e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3.8867e-04 - accuracy: 1.0000 - val_loss: 2.8485e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3.6171e-04 - accuracy: 1.0000 - val_loss: 2.5954e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3.4757e-04 - accuracy: 1.0000 - val_loss: 2.4771e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 3.3385e-04 - accuracy: 1.0000 - val_loss: 2.3992e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3.2212e-04 - accuracy: 1.0000 - val_loss: 2.2538e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3.0671e-04 - accuracy: 1.0000 - val_loss: 2.1435e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3.0708e-04 - accuracy: 1.0000 - val_loss: 2.0279e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 2.8499e-04 - accuracy: 1.0000 - val_loss: 1.9934e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.9934e-04 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "for train_val_index, test_val_index in sss.split(data_Xtrain, data_ytrain):\n",
    "    X_train, X_val = data_Xtrain.iloc[train_val_index], data_Xtrain.iloc[test_val_index]\n",
    "    y_train, y_val = data_ytrain.iloc[train_val_index], data_ytrain.iloc[test_val_index]\n",
    "\n",
    "    train_val_data = X_train\n",
    "    train_val_data = train_val_data.assign(Class=y_train)\n",
    "    print(train_val_data)\n",
    "\n",
    "    fraud_df = train_val_data.loc[train_val_data['Class'] == 1]\n",
    "    non_fraud_df = train_val_data.loc[train_val_data['Class'] == 0].sample(len(fraud_df))\n",
    "    print(f'Relation fraud non-fraud in the CV set: {len(fraud_df)} x {len(non_fraud_df)}')\n",
    "\n",
    "    print(f'Test training relationship {len(notfraud_df)+len(fraud_df)} x {len(y_val)}')\n",
    "    print(f'Test training relationship {(len(notfraud_df)+len(fraud_df))/(len(non_fraud_df)+len(fraud_df)+len(y_val))} x {len(y_val)/(len(non_fraud_df)+len(fraud_df)+len(y_val))}')\n",
    "    \n",
    "    X_train_resampled = pd.concat([fraud_df, non_fraud_df]).drop('Class', axis=1)\n",
    "    y_train_resampled = pd.concat([fraud_df, non_fraud_df])['Class']\n",
    "    \n",
    "    print(X_train_resampled)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Fit the model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    history = model.fit(x=X_train_scaled, y=y_train_resampled, epochs=10, validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=[early_stop])\n",
    "\n",
    "    history_list.append(history.history)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_scaled, y_val)\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    accuracy_scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy Neural Network: 1.0\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1352\n",
      "           1       0.99      0.98      0.99       148\n",
      "\n",
      "    accuracy                           1.00      1500\n",
      "   macro avg       1.00      0.99      0.99      1500\n",
      "weighted avg       1.00      1.00      1.00      1500\n",
      "\n",
      "Recall:  0.9797297297297297\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the average accuracy across all folds\n",
    "print(f\"Average Accuracy Neural Network: {np.mean(accuracy_scores)}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_scaled = scaler.transform(data_Xtest)\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(data_ytest, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_ytest, y_pred))\n",
    "print(\"Recall: \", recall_score(data_ytest, y_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0, 1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming model is already trained and you have X_test and y_test\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(data_ytest, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Find the minimum length of histories\n",
    "min_length = min(len(history['loss']) for history in history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate or pad histories to have the same length\n",
    "truncated_histories = []\n",
    "for history in history_list:\n",
    "    truncated_history = {\n",
    "        'loss': history['loss'][:min_length],\n",
    "        'val_loss': history['val_loss'][:min_length],\n",
    "        'accuracy': history['accuracy'][:min_length],\n",
    "        'val_accuracy': history['val_accuracy'][:min_length]\n",
    "    }\n",
    "    truncated_histories.append(truncated_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
